{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Blaisdell_HW3.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"d20pkki2xx9v","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"jwgRSktrxy8i","colab_type":"text"},"cell_type":"markdown","source":["Marcus Blaisdell\n","Cpt_S 437\n","Homework #3\n","February 12, 2019\n","Dr. Cook\n","\n","1)\n","\n","200 data points\n","100 positive examples\n","100 negative examples\n","d features\n","\n","Use majority class classifier and leave-one-out cross validation:\n","\n","There are two possible ways that leave-one-out will present as:\n","\n","one) \n","the sample left out is positive.\n","This will leave a training set of 199 samples. 99 positive and 100 negative.\n","A majority classifier will therefore predict the solution is negative.\n","This is an incorrect classification on the test sample.\n","\n","two)\n","the sample left out is negative.\n","This will leave a training set of 199 samples. 99 negative and 100 positive.\n","A majority classifier will therefore predict the solution is positive.\n","This is an incorrect classification on the test sample.\n","\n","One of these two options will be the solution for leave-one-out on any of the 200 samples therefore the accuracy will be 0% correct.\n","\n","2)\n","\n","https://docs.google.com/document/d/1rystM5Y2cFHZAcqyYVXVjByM94sLCPtWzPKHGEn5hEs/edit?usp=sharing\n","\n","\n","3)\n","\n","\tperceptron\n","\n","Accuracy of 3-fold =  51.31635276164726\n","Accuracy of LOO:  55.639976621858565\n","\n","\tDecision Trees\n","\n","Accuracy of 3-fold =  48.45024118966418\n","Accuracy of LOO:  58.328462887200466\n","\n","\tk-nearest neighbor\n","\n","Accuracy of 3-fold =  48.50728689792198\n","Accuracy of LOO:  74.63471654003506\n","\n","\tnaive bayes\n","\n","Accuracy of 3-fold =  51.430649010149416\n","Accuracy of LOO:  82.34950321449445\n","\n","The classifier that performs best is the Naive Bayes. It has the highest accuracy for both 3-fold cross validation and Leave-one-out at 51.43% and 82.35% respectively. \n","\n","The worst classifier for 3-fold is the Decision trees with 48.45% accuracy and the worst for LOO is the Perceptron with 55.64% accuracy.\n","\n","I attempted to run the t-test on the arrays of scores of the LOO perceptron and LOO Naive Bayes:\n","\n","t-test\n","\n","t:  5.025971147793566  - p:  5.533492083263338e-07\n","\n","The p value is very small suggesting there is no statistical significance.\n"]},{"metadata":{"id":"eMMxSyiexWSX","colab_type":"code","outputId":"68bee239-908d-4983-8f60-259db39a8683","executionInfo":{"status":"ok","timestamp":1550223677168,"user_tz":480,"elapsed":38491,"user":{"displayName":"Marcus Blaisdell","photoUrl":"https://lh5.googleusercontent.com/-xckPh5sbhZk/AAAAAAAAAAI/AAAAAAAAAE8/-fHm9FgxSOo/s64/photo.jpg","userId":"11116694517108199622"}},"colab":{"base_uri":"https://localhost:8080/","height":442}},"cell_type":"code","source":["import numpy as np\n","from sklearn.linear_model import Perceptron\n","from sklearn import tree\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from scipy import stats\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","\n","### Create a global array to hold all of the data files:\n","\n","fileList = [\"monks-1.csv\", \"monks-2.csv\", \"monks-3.csv\"]\n","\n","tVals1 = []\n","tVals2 = []\n","\n","#########################################################\n","# formatData function:\n","# the monks data is arranged with the labels in the \n","# zero position and the features in the remaining positions\n","# This function reads these files \n","# and returns the attributes and labels\n","# as separate arrays:\n","\n","def formatData (Data):\n","  X = []\n","  y = []\n","\n","  for i in range (len(Data)):\n","    y.append (Data[i][0])\n","    X.append (Data[i][1:len(Data[i])])\n","\n","  return X, y\n","\n","# end formatData function\n","#########################################################\n","\n","\n","\n","#########################################################\n","### Perceptron\n","\n","def runPerceptron ():\n","  #########################################################\n","  \n","  accuracyCount = 0\n","  \n","    ### Print perceptron header:\n","  \n","  print (\"\\n\\tperceptron\\n\")\n","\n","    # run perceptron, max iterations = 50\n","    # using k-fold evaluation:\n","  \n","    ### Create the perceptron:\n","  \n","  clf = Perceptron(tol=1e-3, random_state=0, max_iter=50)\n","  \n","  ### Load the data\n","  \n","  Data = np.loadtxt(fname='/content/gdrive/My Drive/437/HW3/' + fileList[0], delimiter=',')\n","  Data = np.concatenate ((Data, np.loadtxt(fname='/content/gdrive/My Drive/437/HW3/' + fileList[1], delimiter=',')), axis=0)\n","  Data = np.concatenate ((Data, np.loadtxt(fname='/content/gdrive/My Drive/437/HW3/' + fileList[2], delimiter=',')), axis=0)\n","  \n","    ### use k-fold evaluation:\n","\n","    ### pass 1:\n","  #print (\"\\nPass 1:\\n\")\n","\n","  testData = Data[0:570]\n","  trainData = Data[570:1140]\n","\n","      ### Format the Training data:\n","\n","  X, y = formatData (trainData)\n","\n","    ### learn the training data:\n","\n","  clf.fit(X,y)\n","\n","    ### Format the test data:\n","\n","  X, y = formatData (testData)\n","\n","    ### print the results of the test:\n","\n","  #print (\"Accuracy: \", 100 * (clf.score (X,y) ), \"%\\n\" )\n","  accuracyCount += ( 100 * (clf.score (X,y) ) )\n","  \n","  ### pass 2:\n","  #print (\"\\nPass 2:\\n\")\n","\n","  testData = Data[570:1140]\n","  trainData = np.concatenate ((Data[0:570], Data[1140:1711]), axis=0 )\n","\n","      ### Format the Training data:\n","\n","  X, y = formatData (trainData)\n","\n","    ### learn the training data:\n","\n","  clf.fit(X,y)\n","\n","    ### Format the test data:\n","\n","  X, y = formatData (testData)\n","\n","    ### print the results of the test:\n","\n","  #print (\"Accuracy: \", 100 * (clf.score (X,y) ), \"%\\n\" )\n","  accuracyCount += ( 100 * (clf.score (X,y) ) )\n","  \n","  ### pass 3:\n","  #print (\"\\nPass 3:\\n\")\n","\n","  testData = Data[1140:1711]\n","  trainData = Data[0:1140]\n","\n","      ### Format the Training data:\n","\n","  X, y = formatData (trainData)\n","\n","    ### learn the training data:\n","\n","  clf.fit(X,y)\n","\n","    ### Format the test data:\n","\n","  X, y = formatData (testData)\n","\n","    ### print the results of the test:\n","\n","  #print (\"Accuracy: \", 100 * (clf.score (X,y) ), \"%\\n\" )\n","  accuracyCount += ( 100 * (clf.score (X,y) ) )\n","\n","    \n","  print (\"Accuracy of 3-fold = \", float(accuracyCount) / float(3) )\n","  \n","  ### end k-fold loop\n","  #########################################################\n","  \n","  \n","  #########################################################\n","  ### begin leave-one-out\n","  \n","  accuracyCount = 0.0\n","         \n","  for i in range(len(Data)):\n","    trainData = np.delete (Data, i, 0)\n","    testData = Data[i:i+1]\n","    \n","    X, y = formatData(trainData)\n","    \n","    clf.fit (X, y)\n","    \n","    X, y = formatData(testData)\n","    \n","    #print (\"Accuracy: \", 100 * (clf.score (X,y) ), \"%\\n\" )\n","    accuracyCount += 100 * (clf.score (X,y) )\n","    \n","    tVals1.append (100 * (clf.score (X,y)))\n","         \n","  print (\"Accuracy of LOO: \", accuracyCount / float(len(Data)))\n","  \n","  ### end leave-one-out\n","  #########################################################\n","  \n","  \n","  \n","  ### end runPerceptron function \n","  #########################################################\n","  \n","  \n","  \n","#########################################################\n","### Decision Trees:\n","  \n","def runDecisionTrees ():\n","  #########################################################\n","  ### Print decision tree header:\n","  \n","  print (\"\\n\\tDecision Trees\\n\")\n","  \n","  \n","    ### create a decision tree classifier:\n","  \n","  clf = tree.DecisionTreeClassifier ()\n","  \n","    ### use 3-fold evaluation:\n","  \n","  #for i in range (3):\n","    \n","    ### load first training data set:\n","\n","  Data = np.loadtxt(fname='/content/gdrive/My Drive/437/HW3/' + fileList[0], delimiter=',')\n","\n","    ### Decision Trees does not have a partial fit option so we need to \n","    ### load both training datasets at once:\n","\n","  Data = np.concatenate ((Data, np.loadtxt(fname='/content/gdrive/My Drive/437/HW3/' + fileList[1], delimiter=',')), axis=0)\n","  Data = np.concatenate ((Data, np.loadtxt(fname='/content/gdrive/My Drive/437/HW3/' + fileList[2], delimiter=',')), axis=0)\n","\n","  '''\n","    ### Format the data:\n","\n","  X, y = formatData (Data)\n","\n","    ### train:\n","\n","  clf.fit(X,y)\n","\n","    ### load the test data set:\n","\n","  Data = np.loadtxt(fname='/content/gdrive/My Drive/437/HW3/' + fileList[i], delimiter=',')\n","\n","\n","\n","    ### Format the data:\n","\n","  X, y = formatData (Data)\n","\n","    ### print the results of the test:\n","\n","  print (\"train set 1: \", fileList[((i+1)%3)])\n","  print (\"train set 2: \", fileList[((i+2)%3)])\n","  print (\"testset : \", fileList[i])\n","  print (\"Accuracy: \", 100 * (clf.score (X,y) ), \"%\\n\" )\n","  '''\n","  \n","  accuracyCount = 0.0\n","  \n","    ### pass 1:\n","  #print (\"\\nPass 1:\\n\")\n","\n","  testData = Data[0:570]\n","  trainData = Data[570:1140]\n","\n","      ### Format the Training data:\n","\n","  X, y = formatData (trainData)\n","\n","    ### learn the training data:\n","\n","  clf.fit(X,y)\n","\n","    ### Format the test data:\n","\n","  X, y = formatData (testData)\n","\n","    ### print the results of the test:\n","\n","  #print (\"Accuracy: \", 100 * (clf.score (X,y) ), \"%\\n\" )\n","  accuracyCount += ( 100 * (clf.score (X,y) ) )\n","  \n","  ### pass 2:\n","  #print (\"\\nPass 2:\\n\")\n","\n","  testData = Data[570:1140]\n","  trainData = np.concatenate ((Data[0:570], Data[1140:1711]), axis=0 )\n","\n","      ### Format the Training data:\n","\n","  X, y = formatData (trainData)\n","\n","    ### learn the training data:\n","\n","  clf.fit(X,y)\n","\n","    ### Format the test data:\n","\n","  X, y = formatData (testData)\n","\n","    ### print the results of the test:\n","\n","  #print (\"Accuracy: \", 100 * (clf.score (X,y) ), \"%\\n\" )\n","  accuracyCount += ( 100 * (clf.score (X,y) ) )\n","  \n","  ### pass 3:\n","  #print (\"\\nPass 3:\\n\")\n","\n","  testData = Data[1140:1711]\n","  trainData = Data[0:1140]\n","\n","      ### Format the Training data:\n","\n","  X, y = formatData (trainData)\n","\n","    ### learn the training data:\n","\n","  clf.fit(X,y)\n","\n","    ### Format the test data:\n","\n","  X, y = formatData (testData)\n","\n","    ### print the results of the test:\n","\n","  #print (\"Accuracy: \", 100 * (clf.score (X,y) ), \"%\\n\" )\n","  accuracyCount += ( 100 * (clf.score (X,y) ) )\n","\n","    \n","  print (\"Accuracy of 3-fold = \", float(accuracyCount) / float(3) )\n","  \n","  ### end 3-fold loop\n","  #########################################################\n","  \n","  \n","  #########################################################\n","  ### begin leave-one-out\n","  \n","  accuracyCount = 0.0\n","         \n","  for i in range(len(Data)):\n","    trainData = np.delete (Data, i, 0)\n","    testData = Data[i:i+1]\n","    \n","    X, y = formatData(trainData)\n","    \n","    clf.fit (X, y)\n","    \n","    X, y = formatData(testData)\n","    \n","    #print (\"Accuracy: \", 100 * (clf.score (X,y) ), \"%\\n\" )\n","    accuracyCount += 100 * (clf.score (X,y) )\n","         \n","  print (\"Accuracy of LOO: \", accuracyCount / float(len(Data)))  \n","  \n","  ### end leave-one-out\n","  #########################################################\n","  \n","  \n","  \n","  ### end runDecisionTrees function \n","  #########################################################\n","  \n","  \n","  \n","#########################################################\n","### k-nearest neighbor:\n","  \n","def runKNN ():\n","  #########################################################\n","  ### Print knn header:\n","  \n","  print (\"\\n\\tk-nearest neighbor\\n\")\n","  \n","    ### Create a nearest-neighbor classifier\n","  \n","  clf = KNeighborsClassifier (n_neighbors=3)\n","  \n","  ### Use 3-fold:\n","  \n","  #for i in range (3):\n","  \n","    ### Load first data set:\n","\n","  Data = np.loadtxt(fname='/content/gdrive/My Drive/437/HW3/' + fileList[0], delimiter=',')\n","\n","    ### Append the second data set:\n","\n","  Data = np.concatenate ((Data, np.loadtxt(fname='/content/gdrive/My Drive/437/HW3/' + fileList[1], delimiter=',')), axis=0)\n","  Data = np.concatenate ((Data, np.loadtxt(fname='/content/gdrive/My Drive/437/HW3/' + fileList[2], delimiter=',')), axis=0)\n","\n","  '''\n","    ### Format the data:\n","\n","  X, y = formatData (Data)\n","\n","    ### Train:\n","\n","  clf.fit (X, y)\n","\n","    ### Load test data set:\n","\n","  Data = np.loadtxt(fname='/content/gdrive/My Drive/437/HW3/' + fileList[i], delimiter=',')\n","\n","  ### Format the data:\n","\n","  X, y = formatData (Data)\n","\n","    ### calculate prediction accuracy:\n","\n","  correct = 0\n","\n","  for j in range (len(Data)):\n","    if (clf.predict ( [X[j]]) ) == y[j]:\n","      correct += 1\n","\n","    ### print the results of the test:\n","\n","  print (\"train set 1: \", fileList[((i+1)%3)])\n","  print (\"train set 2: \", fileList[((i+2)%3)])\n","  print (\"testset : \", fileList[i])\n","\n","  print (\"Accuracy: \", (100 * (correct / float(len(Data) ) ) ), \"%\\n\")\n","  '''\n","  \n","  accuracyCount = 0.0\n","  \n","    ### pass 1:\n","  #print (\"\\nPass 1:\\n\")\n","\n","  testData = Data[0:570]\n","  trainData = Data[570:1140]\n","\n","      ### Format the Training data:\n","\n","  X, y = formatData (trainData)\n","\n","    ### learn the training data:\n","\n","  clf.fit(X,y)\n","\n","    ### Format the test data:\n","\n","  X, y = formatData (testData)\n","\n","\n","      ### calculate prediction accuracy:\n","\n","  correct = 0\n","\n","  for j in range (len(testData)):\n","    if (clf.predict ( [X[j]]) ) == y[j]:\n","      correct += 1\n","      \n","  accuracyCount += (100 * (correct / float(len(testData) ) ) )\n","  \n","  ### pass 2:\n","  #print (\"\\nPass 2:\\n\")\n","\n","  testData = Data[570:1140]\n","  trainData = np.concatenate ((Data[0:570], Data[1140:1711]), axis=0 )\n","\n","      ### Format the Training data:\n","\n","  X, y = formatData (trainData)\n","\n","    ### learn the training data:\n","\n","  clf.fit(X,y)\n","\n","    ### Format the test data:\n","\n","  X, y = formatData (testData)\n","\n","    ### calculate prediction accuracy:\n","\n","  correct = 0\n","\n","  for j in range (len(testData)):\n","    if (clf.predict ( [X[j]]) ) == y[j]:\n","      correct += 1\n","      \n","  accuracyCount += (100 * (correct / float(len(testData) ) ) )\n","  \n","  ### pass 3:\n","  #print (\"\\nPass 3:\\n\")\n","\n","  testData = Data[1140:1711]\n","  trainData = Data[0:1140]\n","\n","      ### Format the Training data:\n","\n","  X, y = formatData (trainData)\n","\n","    ### learn the training data:\n","\n","  clf.fit(X,y)\n","\n","    ### Format the test data:\n","\n","  X, y = formatData (testData)\n","\n","    ### calculate prediction accuracy:\n","\n","  correct = 0\n","\n","  for j in range (len(testData)):\n","    if (clf.predict ( [X[j]]) ) == y[j]:\n","      correct += 1\n","      \n","  accuracyCount += (100 * (correct / float(len(testData) ) ) )\n","\n","    \n","  print (\"Accuracy of 3-fold = \", float(accuracyCount) / float(3) )\n","  \n","\n","  ### end 3-fold loop\n","  #########################################################\n","  \n","  #########################################################\n","  ### begin leave-one-out\n","  \n","  accuracyCount = 0.0\n","  \n","  for i in range(len(Data)):\n","    trainData = np.delete (Data, i, 0)\n","    testData = Data[i:i+1]\n","    \n","    X, y = formatData(trainData)\n","    \n","    clf.fit (X, y)\n","    \n","    X, y = formatData(testData)\n","    \n","    #print (\"Accuracy: \", 100 * (clf.score (X,y) ), \"%\\n\" )\n","    if (clf.predict ( [X[0]] ) ) == y[0]:\n","      correct += 1\n","      \n","    #accuracyCount += (100 * (correct / float(len(testData) ) ) )\n","         \n","  print (\"Accuracy of LOO: \", ( 100 * (correct / float(len(Data) ) )))  \n","  \n","  ### end leave-one-out\n","  #########################################################\n","  \n","  ### end runKNN function \n","  #########################################################\n","  \n","  \n","  \n","def runNaiveBayes ():\n","  #########################################################\n","  ### Print naive bayes header:\n","  \n","  print (\"\\n\\tnaive bayes\\n\")\n","  \n","    ### Create a naive bayes classifier (using Gaussian):\n","  \n","  clf = GaussianNB ()\n","  \n","  ### Use 3-fold:\n","  \n","  for i in range (3):\n","  \n","      ### Load first data set:\n","\n","    Data = np.loadtxt(fname='/content/gdrive/My Drive/437/HW3/' + fileList[0], delimiter=',')\n","\n","      ### Append the second data set:\n","\n","    Data = np.concatenate ((Data, np.loadtxt(fname='/content/gdrive/My Drive/437/HW3/' + fileList[1], delimiter=',')), axis=0)\n","    Data = np.concatenate ((Data, np.loadtxt(fname='/content/gdrive/My Drive/437/HW3/' + fileList[2], delimiter=',')), axis=0)\n","\n","    '''\n","      ### Format the data:\n","\n","    X, y = formatData (Data)\n","\n","      ### Train:\n","\n","    clf.fit (X, y)\n","    \n","      ### Load test data set:\n","    \n","    Data = np.loadtxt(fname='/content/gdrive/My Drive/437/HW3/' + fileList[i], delimiter=',')\n","    \n","    ### Format the data:\n","    \n","    X, y = formatData (Data)\n","    \n","    '''\n","    \n","    accuracyCount = 0.0\n","  \n","    ### pass 1:\n","  #print (\"\\nPass 1:\\n\")\n","\n","  testData = Data[0:570]\n","  trainData = Data[570:1140]\n","\n","      ### Format the Training data:\n","\n","  X, y = formatData (trainData)\n","\n","    ### learn the training data:\n","\n","  clf.fit(X,y)\n","\n","    ### Format the test data:\n","\n","  X, y = formatData (testData)\n","\n","\n","      ### calculate prediction accuracy:\n","\n","  correct = 0\n","\n","  for j in range (len(testData)):\n","    if (clf.predict ( [X[j]]) ) == y[j]:\n","      correct += 1\n","      \n","  accuracyCount += (100 * (correct / float(len(testData) ) ) )\n","  \n","  ### pass 2:\n","  #print (\"\\nPass 2:\\n\")\n","\n","  testData = Data[570:1140]\n","  trainData = np.concatenate ((Data[0:570], Data[1140:1711]), axis=0 )\n","\n","      ### Format the Training data:\n","\n","  X, y = formatData (trainData)\n","\n","    ### learn the training data:\n","\n","  clf.fit(X,y)\n","\n","    ### Format the test data:\n","\n","  X, y = formatData (testData)\n","\n","    ### calculate prediction accuracy:\n","\n","  correct = 0\n","\n","  for j in range (len(testData)):\n","    if (clf.predict ( [X[j]]) ) == y[j]:\n","      correct += 1\n","      \n","  accuracyCount += (100 * (correct / float(len(testData) ) ) )\n","  \n","  ### pass 3:\n","  #print (\"\\nPass 3:\\n\")\n","\n","  testData = Data[1140:1711]\n","  trainData = Data[0:1140]\n","\n","      ### Format the Training data:\n","\n","  X, y = formatData (trainData)\n","\n","    ### learn the training data:\n","\n","  clf.fit(X,y)\n","\n","    ### Format the test data:\n","\n","  X, y = formatData (testData)\n","\n","    ### calculate prediction accuracy:\n","\n","  correct = 0\n","\n","  for j in range (len(testData)):\n","    if (clf.predict ( [X[j]]) ) == y[j]:\n","      correct += 1\n","      \n","  accuracyCount += (100 * (correct / float(len(testData) ) ) )\n","\n","    \n","  print (\"Accuracy of 3-fold = \", float(accuracyCount) / float(3) )\n","    \n","    \n","  '''\n","      ### calculate prediction accuracy:\n","  \n","    correct = 0\n","  \n","    for j in range (len(Data)):\n","      if (clf.predict ( [X[j]]) ) == y[j]:\n","        correct += 1\n","        \n","      ### print the results of the test:\n","      \n","    print (\"train set 1: \", fileList[((i+1)%3)])\n","    print (\"train set 2: \", fileList[((i+2)%3)])\n","    print (\"testset : \", fileList[i])\n","        \n","    print (\"Accuracy: \", (100 * (correct / float(len(Data) ) ) ), \"%\\n\")\n","  '''\n","  \n","  ### end 3-fold loop\n","  #########################################################\n","  \n","  \n","  #########################################################\n","  ### begin leave-one-out\n","  \n","  accuracyCount = 0.0\n","  \n","  for i in range(len(Data)):\n","    trainData = np.delete (Data, i, 0)\n","    testData = Data[i:i+1]\n","    \n","    X, y = formatData(trainData)\n","    \n","    clf.fit (X, y)\n","    \n","    X, y = formatData(testData)\n","    \n","    #print (\"Accuracy: \", 100 * (clf.score (X,y) ), \"%\\n\" )\n","    if (clf.predict ( [X[0]] ) ) == y[0]:\n","      correct += 1\n","    \n","    tVals2.append (100 * (correct / float(len(Data))))\n","      \n","    #accuracyCount += (100 * (correct / float(len(testData) ) ) )\n","         \n","  print (\"Accuracy of LOO: \", ( 100 * (correct / float(len(Data) ) )))  \n","  \n","  ### end leave-one-out\n","  #########################################################\n","  \n","  \n","  ### end runNaiveBayes function \n","  #########################################################\n","  \n","  \n","#########################################################\n","### t-test:\n","  \n","def runTTest ():\n","  #########################################################\n","  ### Print t-test header:\n","  \n","  print (\"\\n\\tt-test\\n\")\n","  \n","  np.random.seed(1)\n","  \n","  t, p = stats.ttest_rel (tVals1, tVals2)\n","                   \n","  print (\"t: \", t, \" - p: \", p)\n","  \n","  ### end runTTeset function \n","  #########################################################\n","  \n","\n","#########################################################\n","# Main function:\n","\n","if __name__ == \"__main__\":\n","\n","    ### run the perceptron:\n","  \n","  runPerceptron ()\n","  \n","    ### run the decision trees:\n","  \n","  runDecisionTrees ()\n","  \n","  \n","    ### run the nearest neighbors:\n","  \n","  runKNN ()\n","  \n","    ### run the naive bayes:\n","  \n","  runNaiveBayes ()\n","  \n","    ### run the t-test performance measure:\n","  \n","  runTTest ()\n","  \n","  \n","# end Main function  \n","#########################################################"],"execution_count":94,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","\n","\tperceptron\n","\n","Accuracy of 3-fold =  51.31635276164726\n","Accuracy of LOO:  55.639976621858565\n","\n","\tDecision Trees\n","\n","Accuracy of 3-fold =  48.45024118966418\n","Accuracy of LOO:  58.328462887200466\n","\n","\tk-nearest neighbor\n","\n","Accuracy of 3-fold =  48.50728689792198\n","Accuracy of LOO:  74.63471654003506\n","\n","\tnaive bayes\n","\n","Accuracy of 3-fold =  51.430649010149416\n","Accuracy of LOO:  82.34950321449445\n","\n","\tt-test\n","\n","t:  5.025971147793566  - p:  5.533492083263338e-07\n"],"name":"stdout"}]}]}