{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Blaisdell_HW_5.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"HhT2rAX5XFF3","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"u_6qV8HTXFqD","colab_type":"text"},"cell_type":"markdown","source":["Marcus Blaisdell\n","CptS 437\n","Homework #5\n","March 28, 2019\n","Dr. Cook\n","\n","1.\n","\n","Using a quadratic kernel will create a parabolic decision boundary in 2-d space that would otherwise appear as a hyperplane when viewed in three-dimensional space:\n","\n","https://drive.google.com/file/d/1q7byNJvaHgtPsqDlhbMjDyg8LJI7Qbb1/view?usp=sharing\n","\n","2.\n","\n","If we are to produce a single output, y-hat, the two outputs that result from performing the calculations on the given weights, with the given input values, can be combined to produce a single-valued output:\n","\n","https://drive.google.com/file/d/1f5UPMS4Vexny0mEeDWCMef-4oKL7I4Ta/view?usp=sharing\n","\n","3. \n","\n","The hyperparameter that would most dramatically affect the tradeoff between underfitting (high bias) and overfitting (high variance) is the number of hidden nodes.\n","\n","A search of the current literature on the subject offers many suggestions on choosing both the number of hidden layers and the number of nodes in each hidden layer and there is no clear solution to choosing the optimal number for either however, it does seem to be the consensus that increasing the number of hidden nodes has more effect on the ability of the network to successfully make predictions which suggests that it is responsible for greater variation in the ability of the model to fit to the data either optimally, overfit, or underfit.\n","\n","4. \n","\n","See attached code\n","\n","The optimal result seems to be with 96 components, achieving 99.62% accuracy vs. 91.34% accuracy with all 256. \n","One possibility that I believe could be contributing to the higher accuracy with reduced dimensions is that the written digits only occupy percentage of the total space of the entire 16X16 grid which means that the majority (mostly) of the squares are blank or contain no relevant data so by reducing the dimensions we can allow the algorithms to focus on areas that contain relevant data.\n","\n","Another way to explain this is that if we were to superimpose all images of all digits, there would be some areas that would contain no dark spots, most notably the four corners which means that those squares do not contribute to successful classification and reducing the dimensions allows these to become less used in the classification. \n","\n","5.\n","\n","Project Tasks / Timeline:\n","https://drive.google.com/open?id=1kFMnQetqXlVtqjeTjshN6Pa4K4bu5EUXPJrL48J7I2g\n","\n","https://drive.google.com/file/d/11yI7M_xslCwGJ0FeyDoricxxY2sdS-_-/view?usp=sharing\n","\n"]},{"metadata":{"id":"Z0slbTOsNpQA","colab_type":"code","outputId":"6473b197-f20e-4f18-c0e3-04d73d1494a1","executionInfo":{"status":"ok","timestamp":1553752096040,"user_tz":420,"elapsed":9741,"user":{"displayName":"Marcus Blaisdell","photoUrl":"https://lh5.googleusercontent.com/-xckPh5sbhZk/AAAAAAAAAAI/AAAAAAAAAE8/-fHm9FgxSOo/s64/photo.jpg","userId":"11116694517108199622"}},"colab":{"base_uri":"https://localhost:8080/","height":481}},"cell_type":"code","source":["import numpy as np\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.decomposition import PCA\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# create global arrays to store data and labels:\n","dataList = []\n","labelList = []\n","\n","\n","############################################\n","# runMLP function\n","# Multi-Layer Perceptron\n","# Use relu activation function\n","# use alpha = 1e-5\n","\n","def runMLP (theData, theLabel):\n","  clf = MLPClassifier (activation='relu', solver='lbfgs', alpha=1e-5, \n","                      hidden_layer_sizes=(5,2), random_state=1)\n","  \n","  clf.fit (theData, theLabel)\n","  \n","  return clf\n","\n","\n","# end runMLP function\n","############################################\n","\n","\n","############################################\n","# runLR function\n","# Logistic Regression classifer\n","# maximum iterations = 50\n","\n","def runLR (theData, theLabel):\n","  clf = LogisticRegression (random_state=0, solver='lbfgs',\n","                           multi_class='multinomial', max_iter=50)\n","  \n","  clf.fit (theData, theLabel)\n","  \n","  return clf\n","\n","\n","# end runLR function\n","############################################\n","\n","\n","############################################\n","# runRF function\n","# Random Forest classifier\n","# 10 decision trees (n_estimators = 10)\n","\n","\n","def runRF (theData, theLabel):\n","  clf = RandomForestClassifier(n_estimators=10, max_depth=2,\n","                             random_state=0)\n","  \n","  clf.fit (theData, theLabel)\n","  \n","  return clf\n","\n","\n","# end runRF function\n","############################################\n","\n","\n","############################################\n","# getLabel function\n","# accepts an array of 10 characters\n","# and determines which position the '1' is in\n","# and returns that array position as the \n","# digit label:\n","\n","def getLabel (myLabel):\n","  \n","    # initialize with impossible value to detect errors:\n","    \n","  theLabel = 12\n","\n","    # iterate through the list looking for the '1':\n","    # return as soon as we find it (short circuit evaluation)\n","    \n","  for label in range (10):\n","    \n","    if myLabel[label] == '1':\n","      \n","        # if we found the '1'\n","        # return our current position\n","        # as the digit label:\n","        \n","      return label\n","      \n","      # else, return our bogus value 12:\n","      \n","  return theLabel\n","\n","# end getLabel function\n","############################################\n","\n","\n","\n","############################################\n","# getData function\n","# read data from file\n","# split into data and label portions\n","# and input into appropriate\n","# global arrays:\n","\n","def getData ():\n","  \n","    # open the file for read:\n","    \n","  inFile = open('/content/gdrive/My Drive/437/HW5/semeion.data', 'r')\n","  #inFile = open('/content/gdrive/My Drive/437/HW5/Samples.data', 'r')\n","\n","  myString = inFile.readline()\n","\n","  myList = myString.split (\" \")\n","\n","  myData = myList[:-11]\n","\n","  while (myString):\n","\n","    myList = myString.split (\" \")\n","\n","    myData = myList[:-11]\n","    dataList.append (myData)\n","\n","    '''\n","    # Print the data, 16 rows of 16 elements per row:\n","    for i in range (16):\n","      print (\"\")\n","      for j in range (16):\n","        print (myData[(i * 16) + j], \"\",  end=\"\")\n","\n","    print (\"\")\n","    '''\n","\n","      # Convert the labels to a single number, 0-9\n","      # that represents the digit that the sample \n","      # has been labelled as\n","      \n","    myLabel = myList[-11:]\n","    theLabel = getLabel(myLabel)\n","    labelList.append (theLabel)\n","\n","    #print (\"theLabel: \", theLabel)\n","\n","    myString = inFile.readline ()\n","    \n","\n","# end getData function\n","############################################\n","\n","\n","############################################\n","# formatData function\n","# Data is read in as strings,\n","# Need to convert to float:\n","\n","def formatData ():\n","  lineNum = 0\n","  \n","  for line in dataList:\n","    for element in range (len(line)):\n","      dataList[lineNum][element] = float(dataList[lineNum][element])\n","    lineNum += 1\n","\n","\n","# end formatData function\n","############################################\n","\n","\n","#########################################################\n","# myKFolds function:\n","# divides the list into K, mostly equal parts\n","# and returns a list with all of the indexes\n","\n","def myKFolds (theSize, theK):\n","  theList = []\n","  spacing = int (theSize / theK)\n","\n","  for i in range (theK - 1):\n","    theList.append([i * spacing, i * spacing + spacing - 1])\n","\n","  theList.append([(theK - 1) * spacing, theSize])\n","\n","  return theList\n","\n","#\n","#########################################################\n","\n","\n","#########################################################\n","# vote function\n","# accepts 3 predictions, \n","# returns the prediction that occurs the most frequently\n","# if no predictions are in common, return the prediction\n","# of the logistic regression classifier (pred2) because  \n","# experimentation shows that it produces the highest \n","# accuracy (~91% vs. ~68% if we choose either of the\n","# other two classifiers)\n","\n","def vote (pred1, pred2, pred3):\n","  \n","  # if they all match, trivial\n","  \n","  if (pred1 == pred2 == pred3):\n","    return pred1\n","  \n","  # otherwise, do an actual evaluation:\n","  # initialize count1 to 1 because it is the count\n","  # of the first prediction so since there is a first \n","  # prediction (pred1), we already know there is at \n","  # least 1 pred1\n","  # initialize count2 and count3 to zero, only increment them if \n","  # we encounter predictions that don't match pred1\n","  \n","  count1 = 1\n","  count2 = 0\n","  count3 = 0\n","  \n","  if pred2 == pred1:\n","    count1 += 1\n","  else:\n","    count2 += 1\n","    \n","  if pred3 == pred1: \n","    count1 += 1\n","  elif pred3 == pred2: \n","    count2 += 1\n","  else:\n","    count3 += 1\n","    \n","  # if any count is greater than or equal to 2, \n","  # it has the majority of 3 so return that prediction\n","  # but if neither count1 or count2 are 2 or more, \n","  # then there are no common predictions so \n","  # favor logistic regression as the main predictor and return  \n","  # pred2 \n","  \n","  if count1 > 1:\n","    return pred1\n","  elif count2 > 1:\n","    return pred2\n","  else:\n","    return pred2\n","\n","# end vote function \n","#########################################################\n","\n","\n","\n","############################################\n","# main function\n","    \n","def main ():\n","  \n","    # Get the data:\n","    \n","  getData ()\n","  \n","    # format the data:\n","    \n","  formatData ()\n","  \n","  #print (\"data size: \", len(dataList))\n","  #print (\"label size: \", len(labelList))\n","  \n","  print (\"Run with full resolution of 256:\\n\\n\")\n","      # use 3-folds:\n","  \n","  numFolds = 3\n","  \n","  myList = myKFolds (len(dataList), numFolds)\n","  \n","    # Traverse the folds:\n","  \n","  for i in range (len(myList)):\n","  \n","    testData = dataList[myList[i][0]:myList[i][1] + 1]\n","    testLabel = labelList[myList[i][0]:myList[i][1] + 1]\n","    \n","      # initialize trainset with i + 1 % len(list)\n","      # to get the next index in a circular array:\n","  \n","    trainData = dataList[myList[(i + 1) % len(myList)][0]:myList[(i + 1) % len(myList)][1] + 1]\n","    trainLabel = labelList[myList[(i + 1) % len(myList)][0]:myList[(i + 1) % len(myList)][1] + 1]\n","    \n","      # now, concatenate remaining indexes that are not the test index\n","    \n","    for j in range (1, len(myList) - 1):\n","\n","      trainData = np.concatenate ((trainData, dataList[myList[(i + 1 + j) % len(myList)][0]:myList[(i + 1 + j) % len(myList)][1] + 1]), axis=0)\n","      trainLabel = np.concatenate ((trainLabel, labelList[myList[(i + 1 + j) % len(myList)][0]:myList[(i + 1 + j) % len(myList)][1] + 1]), axis=0)\n","\n","      # train the data using each of the classifiers:\n","\n","    mlpCLF = runMLP (dataList, labelList)\n","    lrCLF = runLR (dataList, labelList)\n","    rfCLF = runRF (dataList, labelList)\n","\n","      # test the model using voting:\n","      \n","    correct = 0\n","\n","    for testIndex in range (len(testData)):\n","      \n","        # get the prediction of each classifier:\n","        \n","        pred1 = mlpCLF.predict ([dataList[testIndex]])\n","        pred2 = lrCLF.predict ([dataList[testIndex]])\n","        pred3 = rfCLF.predict ([dataList[testIndex]])\n","        \n","        #print (\"mlp: \", pred1, \" lr: \", pred2, \" rf: \", pred3)\n","        \n","        # get the prediction that has the most votes:\n","          \n","        thePred = vote (pred1, pred2, pred3)\n","  \n","        # evaluate the prediction:\n","    \n","        if thePred == labelList[testIndex]:\n","          correct += 1\n","\n","    theAccuracy = 100 * (float (correct) / len(testData) )\n","    \n","    print (\"Accuracy: \", theAccuracy )\n","    \n","  ### Repeat for PCA:\n","  \n","  print (\"\\n\\nrepeat with dimensionality reduction to 96:\\n\\n\")\n","  \n","  ### PCA:\n","  \n","  ### By experimentation, PCA Value : Accuracy\n","  # 128 : 95.85687382297552\n","  # 96 : 98.68173258003766\n","  # 64 : 97.92843691148776\n","  # 32 : 94.35028248587571\n","  # 16 : 88.51224105461394\n","  \n","  pca = PCA(96)\n","  \n","  newDataList = pca.fit_transform(dataList)\n","\n","      # use 3-folds:\n","  \n","  numFolds = 3\n","  \n","  myList = myKFolds (len(newDataList), numFolds)\n","  \n","    # Traverse the folds:\n","  \n","  for i in range (len(myList)):\n","  \n","    testData = newDataList[myList[i][0]:myList[i][1] + 1]\n","    testLabel = labelList[myList[i][0]:myList[i][1] + 1]\n","    \n","      # initialize trainset with i + 1 % len(list)\n","      # to get the next index in a circular array:\n","  \n","    trainData = newDataList[myList[(i + 1) % len(myList)][0]:myList[(i + 1) % len(myList)][1] + 1]\n","    trainLabel = labelList[myList[(i + 1) % len(myList)][0]:myList[(i + 1) % len(myList)][1] + 1]\n","    \n","      # now, concatenate remaining indexes that are not the test index\n","    \n","    for j in range (1, len(myList) - 1):\n","\n","      trainData = np.concatenate ((trainData, newDataList[myList[(i + 1 + j) % len(myList)][0]:myList[(i + 1 + j) % len(myList)][1] + 1]), axis=0)\n","      trainLabel = np.concatenate ((trainLabel, labelList[myList[(i + 1 + j) % len(myList)][0]:myList[(i + 1 + j) % len(myList)][1] + 1]), axis=0)\n","\n","      # train the data using each of the classifiers:\n","\n","    mlpCLF = runMLP (newDataList, labelList)\n","    lrCLF = runLR (newDataList, labelList)\n","    rfCLF = runRF (newDataList, labelList)\n","\n","      # test the model using voting:\n","      \n","    correct = 0\n","\n","    for testIndex in range (len(testData)):\n","      \n","        # get the prediction of each classifier:\n","        \n","        pred1 = mlpCLF.predict ([newDataList[testIndex]])\n","        pred2 = lrCLF.predict ([newDataList[testIndex]])\n","        pred3 = rfCLF.predict ([newDataList[testIndex]])\n","        \n","        #print (\"mlp: \", pred1, \" lr: \", pred2, \" rf: \", pred3)\n","        \n","        # get the prediction that has the most votes:\n","          \n","        thePred = vote (pred1, pred2, pred3)\n","  \n","        # evaluate the prediction:\n","    \n","        if thePred == labelList[testIndex]:\n","          correct += 1\n","\n","    theAccuracy = 100 * (float (correct) / len(testData) )\n","    \n","    print (\"Accuracy: \", theAccuracy )\n","\n","# end main function\n","############################################\n","\n","\n","\n","\n","if __name__ == \"__main__\":\n","  main ()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Run with full resolution of 256:\n","\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n","  \"of iterations.\", ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy:  91.33709981167608\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n","  \"of iterations.\", ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy:  91.33709981167608\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n","  \"of iterations.\", ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy:  91.33709981167608\n","repeat with dimensionality reduction to 96:\n","\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n","  \"of iterations.\", ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy:  99.62335216572504\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n","  \"of iterations.\", ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy:  99.62335216572504\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n","  \"of iterations.\", ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy:  99.62335216572504\n"],"name":"stdout"}]}]}