{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf200
{\fonttbl\f0\froman\fcharset0 Times-Roman;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\margl1440\margr1440\vieww11600\viewh9460\viewkind0
\deftab720
\pard\pardeftab720\sl280\sa240\partightenfactor0

\f0\fs36 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 As you show, iteration 0 all states have value 0. No actions have been considered and rewards accounted for.\
Iteration 1, the exit action is considered from exit states. This is an immediate reward (no discount) and there are no subsequent states to consider. I agree with your choice of +5 and -5 for the exit states. However, no other states should be updated on this iteration.\uc0\u8232 \
Iteration 2, states more than one step away from an exit state will not have a value change. Exit states will not experience a value change. There are two states adjacent to exit states: (1,2) and (2,2). In each case, consider the maximum-value action from that state. These two states should be updated using the value iteration formula in the slides. For the maximum-value action (in one of the cases this is probably the action that leads to the +5 exit state, consider each of the states that could result from the action (there are three possible outcomes given the transition function). For each outcome, multiply the probability of ending up in that state with the sum of the immediate reward (none if not an exit state) and the discounted value of the resulting state. \uc0\u8232 \
\uc0\u8232 \
\'a0\'a0 Let me know if this addresses the question or if you have remaining concerns.\
}