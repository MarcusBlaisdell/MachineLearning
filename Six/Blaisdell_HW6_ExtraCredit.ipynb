{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Blaisdell_HW6_ExtraCredit.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"lpTzbcp951OJ","colab_type":"code","outputId":"f3aa023a-1b33-426b-aad0-f18f028406a1","executionInfo":{"status":"ok","timestamp":1556327932946,"user_tz":420,"elapsed":107619,"user":{"displayName":"Marcus Blaisdell","photoUrl":"https://lh5.googleusercontent.com/-xckPh5sbhZk/AAAAAAAAAAI/AAAAAAAAAE8/-fHm9FgxSOo/s64/photo.jpg","userId":"11116694517108199622"}},"colab":{"base_uri":"https://localhost:8080/","height":1511}},"cell_type":"code","source":["###########################################\n","### \n","### Marcus Blaisdell\n","### Homework 6, Extra Credit \n","### April 26, 2019\n","### Professor Diane Cook\n","### \n","### Q-Learning and SARSA algorithm\n","### modifications were made in accordance \n","### with the algorithms described here:\n","### https://www.cse.unsw.edu.au/~cs9417ml/RL1/algorithms.html\n","### \n","### The Q-learning and SARSA algorithms\n","### each run in 25-45 seconds each,\n","### the ignorant agent takes ~1 hour to run\n","### \n","### The reporting metrics show similar\n","### average time-steps for all three\n","### functions and the evaluate_solution\n","### function reports no penalties for any\n","### of them using the learned models. \n","### \n","### Changing the movement reward to -4 \n","### did not have an apparent effect but I \n","### had expected it to produce shorter runs\n","### since the higher penalty would create\n","### a higher reward for fewer movements.\n","### \n","###########################################\n","\n","\n","\n","!pip install cmake 'gym[atari'] scipy\n","\n","import gym\n","import numpy as np\n","import random\n","from IPython.display import clear_output\n","import time\n","from time import sleep\n","import random\n","\n","frames = []\n","\n","def init():\n","   env = gym.make(\"Taxi-v2\").env\n","   env.reset() # reset environment to a new random state\n","  \n","   print(\"Action Space {}\".format(env.action_space))\n","   print(\"State Space {}\".format(env.observation_space))\n","   # (taxi row, taxi column, passenger index, destination index)\n","   state = env.encode(3, 1, 2, 0) \n","   print(\"State:\", state)\n","\n","   env.s = state\n","   env.render()\n","   return env\n","\n","def print_frames(frames, firstnum, lastnum):\n","   for i, frame in enumerate(frames):\n","      if i < firstnum or i > (len(frames) - lastnum):\n","        clear_output(wait=True)\n","        print(frame['frame'].getvalue())\n","        print(f\"Timestep: {i + 1}\")\n","        print(f\"State: {frame['state']}\")\n","        print(f\"Action: {frame['action']}\")\n","        print(f\"Reward: {frame['reward']}\")\n","        sleep(0.5)\n","          \n","\n","####################################\n","### begin train_agent\n","\n","def train_agent(env, theReward):\n","   Q = np.zeros([env.observation_space.n, env.action_space.n])\n","   alpha = 0.1\n","   gamma = 0.6\n","   epsilon = 0.1\n","\n","   # For plotting metrics\n","   all_epochs = []\n","   all_penalties = []\n","\n","   for i in range(1, 100001):\n","      state = env.reset()\n","\n","      epochs, penalties, reward, = 0, 0, 0\n","      \n","      done = False\n","      while not done:\n","         if random.uniform(0, 1) < epsilon:\n","            action = env.action_space.sample() # Explore action space\n","         else:\n","            action = np.argmax(Q[state]) # Exploit learned values\n","\n","         next_state, reward, done, info = env.step(action) \n","         if reward == -1:\n","            reward = theReward\n","        \n","         old_value = Q[state, action]\n","         next_max = np.max(Q[next_state])\n","        \n","         #new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n","         ### Using the Q-learning formula from lecture:\n","        \n","         new_value = old_value + (alpha * (reward + (gamma * next_max) - old_value))\n","         Q[state, action] = new_value\n","\n","         if reward == -10:\n","            penalties += 1\n","\n","         state = next_state\n","         epochs += 1\n","          \n","      all_penalties.append (penalties)\n","        \n","   print(\"Training finished.\\n\")\n","   print (\"Reward: \", reward)\n","  \n","   return Q\n","       \n","### end train_agent\n","####################################\n","\n","\n","\n","####################################\n","### begin sarsa\n","\n","def train_agent_sarsa(env, theReward):\n","   Q = np.zeros([env.observation_space.n, env.action_space.n])\n","   alpha = 0.1\n","   gamma = 0.6\n","   epsilon = 0.1\n","\n","   # For plotting metrics\n","   all_epochs = []\n","   all_penalties = []\n","\n","   for i in range(1, 100001):\n","      state = env.reset()\n","\n","      epochs, penalties, reward, = 0, 0, 0\n","      done = False\n","      \n","      \n","        # Choose the action determined by the policy:\n","        \n","      action = np.argmax(Q[state]) \n","      \n","      while not done:\n","            \n","         next_state, reward, done, info = env.step(action) \n","         if reward == -1:\n","            reward = theReward\n","        \n","         old_value = Q[state, action]\n","         next_max = np.max(Q[next_state])\n","        \n","         #new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n","         \n","         ### Using the SARSA formula:\n","          \n","         new_value = old_value + (alpha * (reward + (gamma * next_max) - old_value))\n","         Q[state, action] = new_value\n","\n","         if reward == -10:\n","            penalties += 1\n","\n","         state = next_state\n","         action = np.argmax (Q[state])\n","          \n","         epochs += 1\n","        \n","   print(\"Training finished.\\n\")\n","   print (\"Reward: \", reward)\n","   return Q\n","  \n","### end sarsa\n","####################################\n","\n","\n","####################################\n","### begin ignorant_agent\n","\n","def train_agent_ignorant(env, theReward):\n","   Q = np.zeros([env.observation_space.n, env.action_space.n])\n","   alpha = 0.1\n","   gamma = 0.6\n","   epsilon = 0.1\n","\n","   # For plotting metrics\n","   all_epochs = []\n","   all_penalties = []\n","\n","   for i in range(1, 100001):\n","      state = env.reset()\n","\n","      epochs, penalties, reward, = 0, 0, 0\n","      \n","      done = False\n","      \n","      while not done:\n","         #action = np.argmax(Q[state]) # Exploit learned values\n","          \n","         action = random.randint (0,5)\n","\n","         next_state, reward, done, info = env.step(action) \n","         if reward == -1:\n","            reward = theReward\n","        \n","         old_value = Q[state, action]\n","         \n","         next_max = np.max(Q[next_state])\n","        \n","         #new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n","         new_value = old_value + (alpha * (reward + (gamma * next_max) - old_value))\n","         Q[state, action] = new_value\n","\n","         if reward == -10:\n","            penalties += 1\n","\n","         state = next_state\n","         epochs += 1\n","        \n","   print(\"Training finished.\\n\")\n","   print (\"reward: \", reward)\n","   return Q\n","  \n","### end ignorant_agent\n","####################################\n","    \n","#################################### \n","### begin evaluate_solution \n","\n","def evaluate_solution(env, Q, theReward):\n","   total_epochs, total_penalties = 0, 0\n","    \n","   episodes = 100\n","\n","   for _ in range(episodes):\n","      state = env.reset()\n","      epochs, penalties, reward = 0, 0, 0\n","    \n","      done = False \n","                                  \n","      while not done:\n","         action = np.argmax(Q[state])\n","         state, reward, done, info = env.step(action)\n","         if reward == -1:\n","            reward = theReward\n","         # Put each rendered frame into dict for animation\n","\n","         frames.append({\n","            'frame': env.render(mode='ansi'),\n","            'state': state,\n","            'action': action,\n","            'reward': reward})\n","\n","\n","         if reward == -10:\n","            penalties += 1\n","\n","         epochs += 1\n","\n","      total_penalties += penalties\n","      total_epochs += epochs\n","\n","   print(f\"Results after {episodes} episodes:\")\n","   print (\"total reward: \", reward)\n","   print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n","   print(f\"Average penalties per episode: {total_penalties / episodes}\")\n","  \n","### end evaluate_solution\n","####################################\n","\n","  \n","### learning algorithms: train and test:\n","\n","########################################\n","### begin q_learning:\n","\n","def q_learning(env, theReward):\n","   Q = train_agent(env, theReward)\n","   evaluate_solution(env, Q, theReward)\n","   #return frames\n","\n","### end q_learning:\n","########################################\n","\n","    \n","########################################\n","### begin sarsa_learning:\n","\n","def sarsa(env, theReward):\n","   Q = train_agent_sarsa(env, theReward)\n","   evaluate_solution(env, Q, theReward)\n","   #return frames\n","    \n","### end sarsa_learning:\n","########################################\n","\n","########################################\n","### begin ignorant_agent_learning:\n","\n","def ignorant_agent(env, theReward):\n","   Q = train_agent_ignorant(env, theReward)\n","   evaluate_solution(env, Q, theReward)\n","   #return frames\n","    \n","### end ignorant_agent_learning:\n","########################################\n","\n","    \n","def main ():\n","  env = init()\n","  \n","    ### set animate = 1 to run the animation, \n","    ### a value of 'not 1' will not run the animation\n","  \n","  animate = 0\n","  \n","    ### test with multiple movement rewards\n","  \n","  rewardList = [-1, -4]\n","  \n","  for theReward in rewardList:\n","    \n","    print (\"\\n\\n\\t*** Reward: \", theReward, \"\\n\\n\")\n","\n","    \n","    ###################\n","    ### run q_learning:\n","\n","    print (\"\\t*** q_learning ***\\n\\n\")\n","\n","    qStartTime = time.time ()\n","    q_learning(env, theReward)\n","    qEndTime = time.time ()\n","\n","    print (\"q_learning run time: \", str(qEndTime - qStartTime))\n","\n","    if (print == 1):\n","      print_frames(frames, 10, 10)\n","      \n","    ### end q_learning\n","    ##################\n","    \n","    \n","    #######################\n","    ### run sarsa_learning:\n","    \n","    print (\"\\n\\n\\t*** SARSA ***\\n\\n\")\n","\n","    sStartTime = time.time ()\n","    sarsa(env, theReward)\n","    sEndTime = time.time ()\n","\n","    print (\"SARSA run time: \", sEndTime - sStartTime)\n","    \n","    if (animate == 1):\n","      print_frames(frames, 10, 10)\n","      \n","    ### end sarsa_learning\n","    ######################\n","    \n","    \n","    ######################\n","    ### run ignorant_agent\n","\n","    print (\"\\n\\n\\t*** Ignorant Agent ***\\n\\n\")\n","\n","    iStartTime = time.time ()\n","    ignorant_agent (env, theReward)\n","    iEndTime = time.time ()\n","\n","    print (\"ignorant agent run time: \", str(iEndTime - iStartTime))\n","    \n","    if (print == 1):\n","      print_frames(frames, 10, 10)\n","      \n","    ### end ignorant agent\n","    ######################\n","    \n","  \n","if __name__ == \"__main__\":\n","  main ()\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: cmake in /usr/local/lib/python3.6/dist-packages (3.12.0)\n","Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.10.11)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.2.1)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.16.3)\n","Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (2.21.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.12.0)\n","Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.2)\n","Requirement already satisfied: atari_py>=0.1.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.1.7)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.3.0)\n","Requirement already satisfied: PyOpenGL in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (3.1.0)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (1.24.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (2019.3.9)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (3.0.4)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (2.8)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym[atari]) (0.16.0)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->gym[atari]) (0.46)\n","Action Space Discrete(6)\n","State Space Discrete(500)\n","State: 328\n","+---------+\n","|\u001b[35mR\u001b[0m: | : :G|\n","| : : : : |\n","| : : : : |\n","| |\u001b[43m \u001b[0m: | : |\n","|\u001b[34;1mY\u001b[0m| : |B: |\n","+---------+\n","\n","\n","\n","\t*** Reward:  -1 \n","\n","\n","\t*** q_learning ***\n","\n","\n","Training finished.\n","\n","Reward:  20\n","Results after 100 episodes:\n","total reward:  20\n","Average timesteps per episode: 12.68\n","Average penalties per episode: 0.0\n","q_learning run time:  27.532066345214844\n","\n","\n","\t*** SARSA ***\n","\n","\n","Training finished.\n","\n","Reward:  20\n","Results after 100 episodes:\n","total reward:  20\n","Average timesteps per episode: 12.77\n","Average penalties per episode: 0.0\n","SARSA run time:  23.26818299293518\n","\n","\n","\t*** Reward:  -4 \n","\n","\n","\t*** q_learning ***\n","\n","\n","Training finished.\n","\n","Reward:  20\n","Results after 100 episodes:\n","total reward:  20\n","Average timesteps per episode: 12.43\n","Average penalties per episode: 0.0\n","q_learning run time:  27.895546197891235\n","\n","\n","\t*** SARSA ***\n","\n","\n","Training finished.\n","\n","Reward:  20\n","Results after 100 episodes:\n","total reward:  20\n","Average timesteps per episode: 12.29\n","Average penalties per episode: 0.0\n","SARSA run time:  24.722790956497192\n"],"name":"stdout"}]}]}