{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Blaisdell_Homework_4.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"QVQM3zAkGQvr","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"VZP5-pQWGRNx","colab_type":"text"},"cell_type":"markdown","source":["Marcus Blaisdell\n","CptS 437\n","Homework #4\n","March 5, 2019\n","Professor Cook\n","\n","1)\n","\n","Mean Squared Error:\n","For each point x, calculate prediction, Y_hat by: Y_hat=Theta_0 + Theta_1 * x_i (see linked spreadsheet)\n","Sum all of the Squares of the difference of y and Y_hat, and divide by total number of samples (20)\n","MSE = 18992.12598\n","\n","https://docs.google.com/spreadsheets/d/1sHtyHo-NNmPwknHD-8RsrxPwf1vhQSIzuAEsI0KeNso/edit?usp=sharing\n","\n","Update Theta_0 and Theta_1 by:\n","θ_0=θ_0-α(1/2n ∑_(i=1)^n▒〖(2θ_0+2θ_1 x_i 〗-2y_i))\n","θ_1=θ_1-α(1/2n ∑_(i=1)^n▒〖(2θ_0 x_i 〗-2x_i y_i+〖2θ_1 x〗_i^2))\n","\n","https://drive.google.com/file/d/1_C2jjt7wGNilIOWSzNBYLlLxyxLAbLpg/view?usp=sharing\n","\n","New θ_0 = 7.441607029\n","New θ_1 = 0.003059615741\n","\n","\n","2)\n","\n","The t-value calculates as 3.06. \n","For degrees-of-freedom = 6, p-value is between 2.447 and 3.143 which is between 95% and 98% confidence.\n","As the confidence is greater than 95%, the diffrerence in f-measures for Algorithm 1 and Algorithm 2 are statistically significant. \n","\n","(See link for work)\n","\n","https://docs.google.com/spreadsheets/d/12h75IdNeEiC_HbhcM-m9_Ql13-1W1j5JNOqUkg3Es_Q/edit?usp=sharing\n","\n","\n","3)\n","\n","From the \"Machine Learning Crash Course\" online tutorial at:\n","https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/lambda\n","\n","as lambda increases (tends towards infinity), the model will become more simple. This  would effectively be a higher bias and a lower variance, and as lambda decreases (tends towards zero), the bias will decrease and the variance will increase as the model becomes more complex.\n","\n","The lower lambda value will increase the risk of overfitting the data whereas higher lambda values will have a risk of underfitting the data.\n","\n","\n","\n","4)\n","\n","Our chosen project is stock market value prediction for a single company (Microsoft).\n","\n","Our team has two members: Ngoc Duong and Marcus Blaisdell. We will work on every part of this project together. Thus, there is no specific role for each of us.\n","\n","The data that this project will use is a time-series. There are two possible approaches to this project: a Recurrent Neural Networks (RNN) and a Long, Short-Term Memory Neural Network (LSTM NN). After doing some research, we found that a RNN can only look back to several time steps while a LSTM NN can look back to many time steps. Considering that with the nature of stock market, we believe that a LSTM NN will be the best method to solve this problem.\n","\n","The datasets that we will use will be daily stock information from the past ten years for Microsoft and related companies. We are obtaining our data from NASDAQ.com (National Association of Securities Dealers Automated Quotations).  It is our premise that the stock market value will be affected by other companies that Microsoft works with, and competes against. For example, Intel processors, Dell computers, Apple computers, cloud computing hardware manufacturers. This is a sample list and not comprehensive of the companies we will consider in our project. By analyzing related companies simultaneously, we seek to improve the ability of the ML algorithm to provide a good prediction.\n","\n","We will evaluate the accuracy of the predictions using a variety of time sequences, for example, varying lengths of historical data for training. The performance will be measured using the four main methods of: Accuracy, Precision, Recall, and f1 measure. \n","\n","The feature set will be stock prices and other stock information of Microsoft in the past ten years and a combination of the related companies. We will experiment with combining different companies to test if adding their data improves the prediction of the price of the target company. For example, to compare the history of Intel processors affect on Microsoft prices, the feature set could be: \n","{Microsoft_open, Microsoft_high, Microsoft_low, Microsoft_close, Microsoft_volume, Intel_open, Intel_high, Intel_low, Intel_close, Intel_volume}\n","\n","We will also experiment with adding the weekly, monthly, 3-month averages as a feature to determine if it proves useful as a predictive feature. \n","\n","Working on this project, we hope to learn the basic of stock investment, particularly how stock price is decided or can be predicted. We also want to experience learning a machine learning method (long short term memory approach) by ourselves (without class lectures) and applying what we learnt in a real-world problem. In additional, lately, we have come to know that our chosen project is a different type of machine learning problem (sequential patterns/time series) than what we have learnt in class (classifiers and regressors). Thus, we are excited to explore this new type of machine learning problems and approaches.\n","\n","This information could be useful to experts in both the target company and investment firms. The target company could use the information to adjust their company strategies based on key market indicators. Stock market investors can use this information to adjust their trading plans to optimize their returns. \n","\n","\n","5)\n","\n"]},{"metadata":{"id":"HwfKFwUtkuh2","colab_type":"code","outputId":"65277690-0698-4ed9-e992-76ccf5496240","executionInfo":{"status":"ok","timestamp":1551851602277,"user_tz":480,"elapsed":150743,"user":{"displayName":"Marcus Blaisdell","photoUrl":"https://lh5.googleusercontent.com/-xckPh5sbhZk/AAAAAAAAAAI/AAAAAAAAAE8/-fHm9FgxSOo/s64/photo.jpg","userId":"11116694517108199622"}},"colab":{"base_uri":"https://localhost:8080/","height":918}},"cell_type":"code","source":["import numpy as np\n","import copy\n","import math\n","from sklearn.naive_bayes import GaussianNB\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","#########################################################\n","# formatData function:\n","# the data is arranged with the labels in the \n","# last position and the features in all of the \n","# preceding positions\n","# This function reads the data\n","# and returns the attributes and labels\n","# as separate arrays:\n","\n","\n","def formatData (Data):\n","  X = []\n","  y = []\n","\n","  for i in range (len(Data)):\n","    y.append (Data[i][-1])\n","    X.append (Data[i][:-1])\n","\n","  return X, y\n","\n","# end formatData function\n","#########################################################\n","\n","#########################################################\n","# myKFolds function:\n","# divides the list into K, mostly equal parts\n","# and returns a list with all of the indexes\n","\n","def myKFolds (theSize, theK):\n","  theList = []\n","  spacing = int (theSize / theK)\n","\n","  for i in range (theK - 1):\n","    theList.append([i * spacing, i * spacing + spacing - 1])\n","\n","  theList.append([(theK - 1) * spacing, theSize])\n","\n","  return theList\n","\n","#\n","#########################################################\n","\n","\n","#########################################################\n","# discretize function:\n","# divides the list into 10, equal parts\n","# and returns a list with all of the bucket ranges\n","\n","def discretize (theVal):\n","  if theVal == [0]:\n","    return [0]\n","  else:\n","    theBuckets = []\n","    theSize = theVal / 10\n","\n","    for i in range (9):\n","      theBuckets.append([i * theSize, i * theSize + theSize])\n","\n","    theBuckets.append([9 * theSize, theVal])\n","    \n","    return theBuckets\n","\n","#\n","#########################################################\n","\n","\n","#########################################################\n","# modData function:\n","# Finds the max value of each attribute\n","# and converts each value to a discrete, normalized \n","# value between 0 and 9\n","\n","def modData (Data):\n","    # build a list of attributes:\n","\n","  attributesList = []\n","  \n","    # add a list for each attribute\n","    # we will add the ten buckets to this:\n","    # but first, use it to store the max value:\n","    # exclude the last element, it is the label, \n","    # we don't want to modify it:\n","    \n","  for i in range (len(Data[0]) - 1):\n","    attributesList.append([0])\n","    \n","  # find the max value for each attribute:\n","  # requires reading the entire list through once:\n","  \n","  for i in range (len(Data)):\n","    for j in range (len(Data[0]) - 1):\n","      if Data[i][j] > attributesList[j]:\n","        attributesList[j] = Data[i][j]\n","        \n","  # Now that we have all the max values,\n","  # discretize them and add our buckets in place of our max value:\n","  \n","  for i in range (len(attributesList)):\n","    attributesList[i] = discretize (attributesList[i])\n","      \n","    \n","  # Update all values in original Data with new discretized values:\n","  #print (\"len(Data): \", len(Data))\n","  for i in range (len(Data)):\n","    for j in range (len(attributesList)):\n","      if len(attributesList[j]) == 1:\n","        Data[i][j] = 0\n","        #print (\"newData[i][j]: 0\\n\")\n","      else:\n","        for k in range (10):\n","          #print (\"Data[i][j]: \", Data[i][j])\n","          #print (\"attributesList[j][k][0]: \", attributesList[j][k][0])\n","          #print (\"attributesList[j][k][1]: \", attributesList[j][k][1])\n","          if Data[i][j] == 0:\n","            Data[i][j] = 0\n","            #print (\"newData[i][j]: 0\\n\")\n","          if (Data[i][j] > attributesList[j][k][0]) and (Data[i][j] <= attributesList[j][k][1]):\n","            Data[i][j] = k\n","            #print (\"new Data[i][j]: \", k, \"\\n\")\n","          \n","  return Data\n","      \n","\n","#\n","#########################################################\n","\n","\n","#########################################################\n","# t_test function\n","# calculate the t-value of two sets of scores\n","# to test if they are statistically significant\n","\n","def t_test (aVals, bVals):\n","  \n","    # get the means of each set:\n","  \n","  aMean = getMean (aVals)\n","  bMean = getMean (bVals)\n","  \n","    # get the summation of the deviations:\n","    \n","  summation = getSummation (aVals, bVals, aMean, bMean)\n","  \n","    # N is the number of samples:\n","  \n","  N = len(aVals)\n","  \n","    # Calculate the t-value:\n","  \n","  tValue = (aMean - bMean) * math.sqrt( ((N*(N-1))/summation) )\n","  \n","  return tValue\n","  \n","\n","#\n","#########################################################\n","\n","\n","\n","#########################################################\n","# getMean function\n","# calculates and returns the average value of a list\n","\n","def getMean (theList):\n","  theSum = 0\n","  \n","  for value in theList:\n","    theSum += value\n","    \n","  theSum = float (theSum) / len(theList)\n","  \n","  return theSum\n","\n","#\n","#########################################################\n","\n","\n","#########################################################\n","# getSummation function\n","# calculates and returns the sum of the deviations\n","\n","def getSummation (aVals, bVals, aMean, bMean):\n","  runSum = 0\n","  \n","  for i in range (len(aVals)):\n","    runSum += ((aVals[i] - aMean) - (bVals[i] - bMean))**2\n","    \n","  return runSum\n","\n","#\n","#########################################################\n","\n","\n","\n","#########################################################\n","# Main function:\n","\n","if __name__ == \"__main__\":\n","  \n","  \n","    # create three lists to hold the scores from each function:\n","  \n","  A = []\n","  B = []\n","  C = []\n","  \n","  \n","      # Read data from file:\n","    \n","  Data = np.loadtxt(fname='/content/gdrive/My Drive/437/HW4/smarthome.csv', delimiter=',')\n","  #Data = np.loadtxt(fname='/content/gdrive/My Drive/437/HW4/smarthome-medium.csv', delimiter=',')\n","  #Data = np.loadtxt(fname='/content/gdrive/My Drive/437/HW4/smarthome-small.csv', delimiter=',')\n","\n","\n","  print (\"Part 1:\")\n","  \n","  \n","    # format the data:\n","    \n","  X, y = formatData (Data)\n","\n","  clf = GaussianNB()\n","  \n","    # use 10-folds:\n","  \n","  numFolds = 10\n","  \n","  myList = myKFolds (len(X), numFolds)\n","  \n","    # Traverse the folds:\n","  \n","  for i in range (len(myList)):\n","  \n","    testData = X[myList[i][0]:myList[i][1] + 1]\n","    testLabel = y[myList[i][0]:myList[i][1] + 1]\n","    \n","      # initialize trainset with i + 1 % len(list)\n","      # to get the next index in a circular array:\n","  \n","    trainData = X[myList[(i + 1) % len(myList)][0]:myList[(i + 1) % len(myList)][1] + 1]\n","    trainLabel = y[myList[(i + 1) % len(myList)][0]:myList[(i + 1) % len(myList)][1] + 1]\n","    \n","      # now, concatenate remaining indexes that are not the test index\n","    \n","    for j in range (1, len(myList) - 1):\n","\n","      trainData = np.concatenate ((trainData, X[myList[(i + 1 + j) % len(myList)][0]:myList[(i + 1 + j) % len(myList)][1] + 1]), axis=0)\n","      trainLabel = np.concatenate ((trainLabel, y[myList[(i + 1 + j) % len(myList)][0]:myList[(i + 1 + j) % len(myList)][1] + 1]), axis=0)\n","\n","      # train the model:\n","      \n","    clf.fit(trainData,trainLabel)\n","\n","      # test the model:\n","      \n","    correct = 0\n","\n","    for testIndex in range (len(testData)):\n","      #print (\"clf.predict_proba ( [testData[testIndex]]) ): \", clf.predict_proba ( [testData[testIndex]]) ) \n","      if (clf.predict ( [testData[testIndex]]) ) == testLabel[testIndex]:\n","        correct += 1\n","\n","    theAccuracy = 100 * (float (correct) / len(testData) )\n","    \n","    print (\"Accuracy: \", theAccuracy )\n","    \n","    A.append (theAccuracy)\n","    \n","  \n","  ### End part 1\n","  ########################################################################\n","  \n","  print (\"Part 2:\")\n","\n","  Data = modData (Data)\n","  \n","  #print (\"Data[0]: \", Data[0])\n","  \n","      # format the data:\n","    \n","  X, y = formatData (Data)\n","\n","  clf = GaussianNB()\n","  \n","    # use 10-folds:\n","  \n","  numFolds = 10\n","  \n","  myList = myKFolds (len(X), numFolds)\n","  \n","    # Traverse the folds:\n","  \n","  for i in range (len(myList)):\n","  \n","    testData = X[myList[i][0]:myList[i][1] + 1]\n","    testLabel = y[myList[i][0]:myList[i][1] + 1]\n","    \n","      # initialize trainset with i + 1 % len(list)\n","      # to get the next index in a circular array:\n","  \n","    trainData = X[myList[(i + 1) % len(myList)][0]:myList[(i + 1) % len(myList)][1] + 1]\n","    trainLabel = y[myList[(i + 1) % len(myList)][0]:myList[(i + 1) % len(myList)][1] + 1]\n","    \n","      # now, concatenate remaining indexes that are not the test index\n","    \n","    for j in range (1, len(myList) - 1):\n","\n","      trainData = np.concatenate ((trainData, X[myList[(i + 1 + j) % len(myList)][0]:myList[(i + 1 + j) % len(myList)][1] + 1]), axis=0)\n","      trainLabel = np.concatenate ((trainLabel, y[myList[(i + 1 + j) % len(myList)][0]:myList[(i + 1 + j) % len(myList)][1] + 1]), axis=0)\n","\n","      # train the model:\n","      \n","    clf.fit(trainData,trainLabel)\n","\n","      # test the model:\n","      \n","    correct = 0\n","\n","    for testIndex in range (len(testData)):\n","      if (clf.predict ( [testData[testIndex]]) ) == testLabel[testIndex]:\n","        correct += 1\n","\n","    theAccuracy = 100 * (float (correct) / len(testData) )\n","    print (\"Accuracy: \",  theAccuracy)\n","    \n","    B.append (theAccuracy)\n","    \n","  \n","  ### End part 2\n","  ########################################################################\n","  \n","  \n","  print (\"Part 3:\")\n","  \n","  \n","    # create an integer to hold the prediction from the 8 models\n","    \n","  thePrediction = 0\n","  \n","    # format the data:\n","    \n","  X, y = formatData (Data)\n","\n","  clf_0 = GaussianNB()\n","  clf_1 = GaussianNB()\n","  clf_2 = GaussianNB()\n","  clf_3 = GaussianNB()\n","  clf_4 = GaussianNB()\n","  clf_5 = GaussianNB()\n","  clf_6 = GaussianNB()\n","  clf_7 = GaussianNB()\n","  \n","    # use 10-folds:\n","  \n","  numFolds = 10\n","  \n","  myList = myKFolds (len(X), numFolds)\n","  \n","    # Traverse the folds:\n","  \n","  for i in range (len(myList)):\n","  \n","    testData = X[myList[i][0]:myList[i][1] + 1]\n","    testLabel = y[myList[i][0]:myList[i][1] + 1]\n","    \n","      # initialize trainset with i + 1 % len(list)\n","      # to get the next index in a circular array:\n","  \n","    trainData = X[myList[(i + 1) % len(myList)][0]:myList[(i + 1) % len(myList)][1] + 1]\n","    trainLabel = y[myList[(i + 1) % len(myList)][0]:myList[(i + 1) % len(myList)][1] + 1]\n","    \n","      # now, concatenate remaining indexes that are not the test index\n","    \n","    for j in range (1, len(myList) - 1):\n","\n","      trainData = np.concatenate ((trainData, X[myList[(i + 1 + j) % len(myList)][0]:myList[(i + 1 + j) % len(myList)][1] + 1]), axis=0)\n","      trainLabel = np.concatenate ((trainLabel, y[myList[(i + 1 + j) % len(myList)][0]:myList[(i + 1 + j) % len(myList)][1] + 1]), axis=0)\n","\n","    \n","    trainLabel_0 = copy.deepcopy(trainLabel)\n","    trainLabel_1 = copy.deepcopy(trainLabel)\n","    trainLabel_2 = copy.deepcopy(trainLabel)\n","    trainLabel_3 = copy.deepcopy(trainLabel)\n","    trainLabel_4 = copy.deepcopy(trainLabel)\n","    trainLabel_5 = copy.deepcopy(trainLabel)\n","    trainLabel_6 = copy.deepcopy(trainLabel)\n","    trainLabel_7 = copy.deepcopy(trainLabel)\n","    \n","    testLabel_0 = copy.deepcopy(testLabel)\n","    testLabel_1 = copy.deepcopy(testLabel)\n","    testLabel_2 = copy.deepcopy(testLabel)\n","    testLabel_3 = copy.deepcopy(testLabel)\n","    testLabel_4 = copy.deepcopy(testLabel)\n","    testLabel_5 = copy.deepcopy(testLabel)\n","    testLabel_6 = copy.deepcopy(testLabel)\n","    testLabel_7 = copy.deepcopy(testLabel)\n","    \n","    # convert the labels for one-versus-all:\n","    \n","    for index in range(len(trainLabel_0)):\n","      if trainLabel_0[index] != 0:\n","        trainLabel_0[index] = 9\n","        \n","    for index in range(len(trainLabel_1)):\n","      if trainLabel_1[index] != 1:\n","        trainLabel_1[index] = 9\n","            \n","    for index in range(len(trainLabel_2)):\n","      if trainLabel_2[index] != 2:\n","        trainLabel_2[index] = 9\n","        \n","    for index in range(len(trainLabel_3)):\n","      if trainLabel_3[index] != 3:\n","        trainLabel_3[index] = 9\n","        \n","    for index in range(len(trainLabel_4)):\n","      if trainLabel_4[index] != 4:\n","        trainLabel_4[index] = 9\n","        \n","    for index in range(len(trainLabel_5)):\n","      if trainLabel_5[index] != 5:\n","        trainLabel_5[index] = 9\n","        \n","    for index in range(len(trainLabel_6)):\n","      if trainLabel_6[index] != 6:\n","        trainLabel_6[index] = 9      \n","        \n","    for index in range(len(trainLabel_7)):\n","      if trainLabel_7[index] != 7:\n","        trainLabel_7[index] = 9\n","        \n","    for index in range(len(testLabel_0)):\n","      if testLabel_0[index] != 0:\n","        testLabel_0[index] = 9\n","        \n","    for index in range(len(testLabel_1)):\n","      if testLabel_1[index] != 1:\n","        testLabel_1[index] = 9        \n","        \n","    for index in range(len(testLabel_2)):\n","      if testLabel_2[index] != 2:\n","        testLabel_2[index] = 9\n","        \n","    for index in range(len(testLabel_3)):\n","      if testLabel_3[index] != 3:\n","        testLabel_3[index] = 9\n","        \n","    for index in range(len(testLabel_4)):\n","      if testLabel_4[index] != 4:\n","        testLabel_4[index] = 9\n","        \n","    for index in range(len(testLabel_5)):\n","      if testLabel_5[index] != 5:\n","        testLabel_5[index] = 9\n","        \n","    for index in range(len(testLabel_6)):\n","      if testLabel_6[index] != 6:\n","        testLabel_6[index] = 9\n","        \n","    for index in range(len(testLabel_7)):\n","      if testLabel_7[index] != 7:\n","        testLabel_7[index] = 9\n","        \n","        # train the model:\n","      \n","    clf_0.fit(trainData,trainLabel_0)\n","    clf_1.fit(trainData,trainLabel_1)\n","    clf_2.fit(trainData,trainLabel_2)\n","    clf_3.fit(trainData,trainLabel_3)\n","    clf_4.fit(trainData,trainLabel_4)\n","    clf_5.fit(trainData,trainLabel_5)\n","    clf_6.fit(trainData,trainLabel_6)\n","    clf_7.fit(trainData,trainLabel_7)\n","\n","      # test the model:\n","      \n","    correct = 0\n","\n","    for testIndex in range (len(testData)):\n","      #pred_0 = clf_0.predict_proba ( [testData[testIndex]])[0][0]\n","      #print (\"showVal: \", pred_0, \" - testLabel_0[testIndex]: \", testLabel_0[testIndex])\n","        \n","        # get max of all 8 scores for this point:\n","        # set maxScoreIndex to zero initially,\n","        # if a higher value is found from any of the remaining 7 scores,\n","        # replace the max score with that score,\n","        # and update the predicted value, thePrediction\n","        \n","      score_0 = clf_0.predict_proba ( [testData[testIndex]])[0][0]\n","      maxScore = score_0\n","      thePrediction = 0\n","      \n","      score_1 = clf_1.predict_proba ( [testData[testIndex]])[0][0]\n","      if score_1 > maxScore:\n","        maxScore = score_1\n","        thePrediction = 1\n","        \n","      score_2 = clf_2.predict_proba ( [testData[testIndex]])[0][0]\n","      if score_2 > maxScore:\n","        maxScore = score_2\n","        thePrediction = 2\n","      \n","      score_3 = clf_3.predict_proba ( [testData[testIndex]])[0][0]\n","      if score_3 > maxScore:\n","        maxScore = score_3\n","        thePrediction = 3\n","        \n","      score_4 = clf_4.predict_proba ( [testData[testIndex]])[0][0]\n","      if score_4 > maxScore:\n","        maxScore = score_4\n","        thePrediction = 4\n","        \n","      score_5 = clf_5.predict_proba ( [testData[testIndex]])[0][0]\n","      if score_5 > maxScore:\n","        maxScore = score_5\n","        thePrediction = 5\n","        \n","      score_6 = clf_6.predict_proba ( [testData[testIndex]])[0][0]\n","      if score_6 > maxScore:\n","        maxScore = score_6\n","        thePrediction = 6\n","        \n","      score_7 = clf_7.predict_proba ( [testData[testIndex]])[0][0]\n","      if score_7 > maxScore:\n","        maxScore = score_7\n","        thePrediction = 7\n","      \n","      if thePrediction == testLabel[testIndex]:\n","        correct += 1\n","\n","    theAccuracy = 100 * (float (correct) / len(testData) )\n","    \n","    print (\"Accuracy: \", theAccuracy )\n","    \n","    C.append(theAccuracy)\n","    \n","    \n","  ### End part 3\n","  ########################################################################\n","\n","    \n","  tScoreAB = t_test (A, B)\n","  tScoreAC = t_test (A, C)\n","  tScoreBC = t_test (B, C)\n","  \n","  print (\"\\n\\nFor Degrees-of-Freedom = 9, a t-value greater than 2.262\")\n","  print (\"has a confidence level greater than 95%\")\n","  print (\"and is therefor Statistically significant\\n\\n\")\n","  \n","  print (\"\\n\\nt-score for Part 1 and Part 2: \", tScoreAB )\n","  if tScoreAB > 2.262:\n","    print (\"\\tStatistically Significant\")\n","  else:\n","    print (\"\\tnot Statistically Significant\")\n","  print (\"\\n\\nt-score for Part 1 and Part 3: \", tScoreAC )\n","  if tScoreAC > 2.262:\n","    print (\"\\tStatistically Significant\")\n","  else:\n","    print (\"\\tnot Statistically Significant\")\n","  print (\"\\n\\nt-score for Part 2 and Part 3: \", tScoreBC )\n","  if tScoreBC > 2.262:\n","    print (\"\\tStatistically Significant\")\n","  else:\n","    print (\"\\tnot Statistically Significant\")\n","  \n","# end Main function  \n","#########################################################\n","  "],"execution_count":34,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Part 1:\n","Accuracy:  84.0054548996688\n","Accuracy:  83.81063705435417\n","Accuracy:  78.92070913695694\n","Accuracy:  81.80401324761348\n","Accuracy:  85.36917981687122\n","Accuracy:  84.9600623417105\n","Accuracy:  86.92772257938827\n","Accuracy:  88.4473017728424\n","Accuracy:  83.86908240794857\n","Accuracy:  90.9727626459144\n","Part 2:\n","Accuracy:  81.97934930839665\n","Accuracy:  80.32339762322229\n","Accuracy:  74.34248977206312\n","Accuracy:  75.02435223066432\n","Accuracy:  78.97915449055134\n","Accuracy:  78.0440288330411\n","Accuracy:  82.99240210403272\n","Accuracy:  72.41379310344827\n","Accuracy:  72.86187414767193\n","Accuracy:  73.59922178988327\n","Part 3:\n","Accuracy:  78.53107344632768\n","Accuracy:  76.6218585622443\n","Accuracy:  69.37463471654004\n","Accuracy:  73.2904734073641\n","Accuracy:  78.43366452367036\n","Accuracy:  79.25189947399181\n","Accuracy:  78.08299240210404\n","Accuracy:  77.40112994350282\n","Accuracy:  65.67309565556205\n","Accuracy:  75.03891050583658\n","\n","\n","For Degrees-of-Freedom = 9, a t-value greater than 2.262\n","have a confidence level greater than 95%\n","and is therefor Statistically significant\n","\n","\n","\n","\n","t-score for Part 1 and Part 2:  4.704413186570431\n","\tStatistically Significant\n","\n","\n","t-score for Part 1 and Part 3:  7.242246512684429\n","\tStatistically Significant\n","\n","\n","t-score for Part 2 and Part 3:  1.61864722475976\n","\tnot Statistically Significant\n"],"name":"stdout"}]}]}