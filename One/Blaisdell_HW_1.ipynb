{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Blaisdell_HW_1.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"ySOME80uc47T","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"xwY_hy9uc6ji","colab_type":"text"},"cell_type":"markdown","source":["Marcus Blaisdell\n","Cpt_S 437\n","Homework #1\n","January 17, 2019\n","Dr. Cook\n","\n","1.\n","![alt text](https://drive.google.com/file/d/1CsE8ad6cPfto8uyGveM4aSuVrEOFAzMK/view?usp=sharing)\n","\n","If the above link does not work, below is the text of the shared link to the image on my google drive:\n","\n","https://drive.google.com/file/d/1CsE8ad6cPfto8uyGveM4aSuVrEOFAzMK/view?usp=sharing\n","\n","1. The ‘o’ on the right-hand set is more circular than the one on the left-hand set\n","2. The ‘u’ has a slant to the right on the right-hand set, it is more vertical on the left-hand set\n","3. The ‘C’ has a straighter edge along the left side of the curve on the left-hand set and is more curved on the right-hand set\n","4. The closed loop on the ‘g’ is larger on the right-hand set than the left-hand set.\n","5. The ‘o’ is larger on the right-hand set than the left-hand set\n","6. The tail of the ‘g’ exits the loop more to the bottom on the left-hand set and more to the right on the right-hand set\n","7. The start of the ’s’ loop is more to the right on the right-hand set, it extends more to the end of the lower loop than it does on the left-hand set.\n","8. The ‘C’ curls down more frequently on the right-hand set than it does on the left-hand set.\n","9. The bottom curve of the ‘u’ is sharper on the left-hand set than the right-hand set.\n","10. The lines of the right-hand set are slightly thicker/darker than the left-hand set.\n","\n","2.\n","\n","![alt text](https://drive.google.com/file/d/15Lnw73sv61gmRy0s2Q1CaJNpf1mz3LYU/view?usp=sharing)\n","\n","If the above link does not work, below is the text of the shared link to the image on my google drive:\n","\n","https://drive.google.com/file/d/15Lnw73sv61gmRy0s2Q1CaJNpf1mz3LYU/view?usp=sharing\n","\n","\n","3.\n","\n","3/6 = +\n","3/6 = -\n","\n","E(s) = -(3/6)log_2(3/6) -(3/6)log_2(3/6) = 1.0\n","\n","Gain(F1) = E(s) - ( (2/6)E(a) + (2/6)E(b) + (2/6)E(c) )\n","E(a) = -(1/2)log_2(1/2) - (1/2)log_2(1/2) = 1.0\n","E(b) = -(0/2)log_2(0/2) - (2/2)log_2(2/2) = 0.0\n","E(c) = -(2/2)log_2(2/2) - (0/2)log_2(0/2) = 0.0\n","Gain(F1) = 1.0 - (2/6)(1.0) - (2/6)(0.0) - (2/6)(0.0) = (4/6) = (2/3) = 0.667\n","\n","Gain(F2) = E(s) - ( (3/6)E(a) + (3/6)E(b) + (0/6)E(c) )\n","E(a) = -(2/3)log_2(2/3) - (1/3)log_2(1/3) = 0.918\n","E(b) = -(1/3)log_2(1/3) - (2/3)log_2(2/3) = 0.918\n","E(c) = -(0/0)log_2(0/0) - (0/0)log_2(0/0) = 0.0\n","Gain(F2) = 1.0 - (3/6)(0.918) - (3/6)(0.918) - (0/6)(0.0) = 0.082\n","\n","Gain(F3) = E(s) - ( (2/6)E(a) + (0/6)E(b) + (4/6)E(c) )\n","E(a) = -(1/2)log_2(1/2) - (1/2)log_2(1/2) = 1.0\n","E(b) = -(0/0)log_2(0/0) - (0/0)log_2(0/0) = 0.0\n","E(c) = -(2/4)log_2(2/4) - (2/4)log_2(2/4) = 1.0\n","Gain(F3) = 1.0 - (2/6)(1.0) - (0/6)(0.0) - (4/6)(1.0) = 0.0\n","\n","Information gain is greatest with F1 as the root node so F1 will be selected as the root node of the decision tree\n","\n","\n","4. \n","\n","If we define overfit as the difference between the training accuracy and the testing accuracy, then:\n","Overfit = Train_Accuracy - Test_Accuracy\n","and a greater difference between the training accuracy and the testing accuracy would describe a greater overfit. \n","\n","In the provided graph, the overfit is the least for decision trees of size 10 to size 20. It seems reasonable to predict that a larger number of training examples will allow for better generalization to the test data up to a point so I would choose the decision tree of size 20 to get the best generalization to the testing data without overfitting.\n","\n","\n","5.\n","The performance time increases substantially as the data size increases. \n","I first tested this with a small set of only 40 rows and it completed in less than one second.\n","I then tested it with a set of 500 rows and it completed in < 2 minutes.\n","A set of 1000 rows took ~8 minutes. \n","I let it run for one hour with the full dataset and it was still running.\n","The time performance is very poor at scale.\n","\n","The prediction success consistently reported 100% for datasets of 40, 500, and 1000 (which is suspicious) but based on that, the algorithm greatly outperforms random guess which would be expected to have approximately a 50% success rate. \n"]},{"metadata":{"id":"lwYpvWUfN2lf","colab_type":"code","outputId":"ddf5ae06-9aa6-4f2d-995f-adf131e82960","executionInfo":{"status":"ok","timestamp":1547795630188,"user_tz":480,"elapsed":419078,"user":{"displayName":"Marcus Blaisdell","photoUrl":"https://lh5.googleusercontent.com/-xckPh5sbhZk/AAAAAAAAAAI/AAAAAAAAAE8/-fHm9FgxSOo/s64/photo.jpg","userId":"11116694517108199622"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"cell_type":"code","source":["# Number 5:\n","\n","import math\n","import numpy\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# countClass: get a count of each of the classifications\n","# to use as the pos, neg, that entropy and gain expect:\n","def countClass (groups):\n","  pos = 0\n","  neg = 0\n","  for list in groups:\n","    for row in list:\n","      if row[-1] == 0:\n","        neg += 1\n","      else:\n","        pos += 1\n","        \n","  return pos, neg\n","\n","# countList:\n","def countList (myList):\n","  pos = 0\n","  neg = 0\n","  for row in myList:\n","    if row[-1] == 0:\n","      neg += 1\n","    else:\n","      pos += 1\n","      \n","  return pos, neg\n","\n","# calcGain\n","def calcGain (groups, classes):\n","  # get the count of pos and neg values:\n","  pos, neg = countClass (groups)\n","  left, right = calcSplits (groups)\n","  splits = [left, right]\n","  '''\n","  print ('splits:')\n","  print ('left: (', left[0], ', ', left[1], ')')\n","  print ('right: (', right[0], ', ', right[1], ')')\n","  '''\n","  gain (pos, neg, splits)\n","  '''\n","  print ('calcGain:')\n","  print ('pos: ', pos)\n","  print ('neg: ', neg)\n","  '''\n","\n","# calcSplits\n","def calcSplits (groups):\n","  left = [0,0]\n","  right = [0,0]\n","  left[0], left[1] = countList (groups[0])\n","  right[0], right[1] = countList (groups[1])\n","  '''\n","  print ('calcSplits: ')\n","  print ('left: (', left[0], ', ', left[1], ')')\n","  print ('right: (', right[0], ', ', right[1], ')')\n","  '''\n","  return left, right\n","  \n","  \n","# Calculate the entropy given a set of positive and negative values:\n","# positive (pos) is sitting\n","# negative (neg) is walking\n","\n","def entropy (pos, neg):\n","  if (pos + neg) == 0:\n","    pf = 0\n","    nf = 0\n","  else:\n","    pf = pos / float (pos + neg)\n","    nf = neg / float (pos + neg)\n","  if pf == 0:\n","    term1 = 0\n","  else:\n","    term1 = -pf * math.log(pf, 2.0)\n","  if nf == 0:\n","    term2 = 0\n","  else:\n","    term2 = -nf * math.log (nf, 2.0)\n","  entropy = term1 + term2\n","  return entropy\n","\n","# calculate the information gain given a set of positive and negative values\n","# and the splits values of a decision tree\n","# positive (pos) is sitting\n","# negative (neg) is walking\n","\n","def gain (pos, neg, splits):\n","  start = entropy (pos, neg)\n","  #print ('start', start)\n","  sum = start \n","  for feature in splits:\n","    #print ('feature', feature)\n","    size = (feature[0] + feature[1]) / (pos + neg)\n","    feature_entropy = entropy (feature[0], feature[1])\n","    #print ('feature_entropy: ', feature_entropy)\n","    sum -= size * feature_entropy\n","  return sum\n","\n","# Calculate the Gini index for a subset of the dataset\n","def gini_index(groups, classes):\n","   # count all samples at split point\n","   num_instances = float(sum([len(group) for group in groups]))\n","\n","   gini = 0.0 # sum weighted Gini index for each group\n","   for group in groups:\n","      size = float(len(group))\n","      if size == 0: # avoid divide by zero\n","         continue\n","      score = 0.0\n","      # score the group based on the score for each class\n","      for class_val in classes:\n","         p = [row[-1] for row in group].count(class_val) / size\n","         score += p * p\n","      # weight the group score by its relative size\n","      gini += (1.0 - score) * (size / num_instances)\n","   return gini\n","\n","# Create child splits for a node or make a leaf node\n","def split(node, max_depth, depth):\n","   left, right = node['groups']\n","   del(node['groups'])\n","   # check for a no split\n","   if not left or not right:\n","      node['left'] = node['right'] = create_leaf(left + right)\n","      return\n","   # check for max depth\n","   if depth >= max_depth:\n","      node['left'], node['right'] = create_leaf(left), create_leaf(right)\n","      return\n","   node['left'] = select_attribute(left)\n","   split(node['left'], max_depth, depth+1)\n","   node['right'] = select_attribute(right)\n","   split(node['right'], max_depth, depth+1)\n","\n","\n","# split the dataset based on an attribute and attribute value\n","def test_split(index, value, dataset):\n","   left, right = list(), list()\n","   for row in dataset:\n","      if row[index] < value:\n","         left.append(row)\n","      else:\n","         right.append(row)\n","      \n","   '''\n","   print ('left:')\n","   print (left)\n","   print ('right:')\n","   print (right)\n","   '''\n","   return left, right\n","\n","\n","# Select the best split point for a dataset\n","def select_attribute(dataset):\n","   class_values = list(set(row[-1] for row in dataset))\n","   b_index, b_value, b_score, b_groups = 999, 999, 999, None\n","   for index in range(len(dataset[0])-1):\n","      for row in dataset:\n","         groups = test_split(index, row[index], dataset)\n","         #print ('row[', index, ']:', row[index])\n","         '''\n","         print ('groups:')\n","         print (groups)\n","         print ('class_values:')\n","         print (class_values)\n","         '''\n","         calcGain (groups, class_values)\n","         gini = gini_index(groups, class_values)\n","         if gini < b_score:\n","            b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n","   return {'index':b_index, 'value':b_value, 'groups':b_groups}\n","\n","# Create a leaf node class value\n","def create_leaf(group):\n","   outcomes = [row[-1] for row in group]\n","   return max(set(outcomes), key=outcomes.count)\n","\n","# Build a decision tree\n","def build_tree(train, max_depth):\n","   root = select_attribute(train)\n","   split(root, max_depth, 1)\n","   return root\n","  \n","  \n","# Print a decision tree\n","def print_tree(node, depth=0):\n","   if depth == 0:\n","      print ('Tree:')\n","   if isinstance(node, dict):\n","      print('%s[X%d < %.3f]' % ((depth*' ', (node['index']+1), node['value'])))\n","      print_tree(node['left'], depth+1)\n","      print_tree(node['right'], depth+1)\n","   else:\n","      print('%s[%s]' % ((depth*' ', node)))\n","      \n","      \n","if __name__ == \"__main__\":\n","   dataset = numpy.loadtxt(fname='/content/gdrive/My Drive/437/HW1/har.csv', delimiter=',')\n","   # training data will be 2 of every 3 elements of the dataset:\n","   trainData = numpy.array([dataset[0]])\n","\n","   #for x in range (1, int (len(dataset) / 3) ):\n","   #for x in range (1, 1000 ):\n","   for x in range (1, len(dataset ):               \n","      if (x + 1) % 3 != 0:\n","        trainData = numpy.append (trainData, [dataset[x]], axis = 0)\n","        \n","   #print ('trainData - ', len(trainData), ': ')\n","   #print (trainData)\n","   '''\n","   dataset = [[2.771244718,1.784783929,0], [1.728571309,1.169761413,0],\n","              [3.678319846,2.812813570,0], [3.961043357,2.619950320,0],\n","              [2.999208922,2.209014212,0], [7.497545867,3.162953546,1],\n","              [9.00220326, 3.339047188,1], [7.444542326,0.476683375,1],\n","              [10.12493903,3.234550982,1], [6.642287351,3.319983761,1]]\n","   '''\n","   #tree = build_tree(dataset, 1)\n","   tree = build_tree(trainData, 1)\n","   print_tree(tree)\n","    \n","# Make a prediction with a decision tree\n","def predict(node, row):\n","   if row[node['index']] < node['value']:\n","      if isinstance(node['left'], dict):\n","         return predict(node['left'], row)\n","      else:\n","         return node['left']\n","   else:\n","      if isinstance(node['right'], dict):\n","         return predict(node['right'], row)\n","      else:\n","         return node['right']\n","\n","        \n","if __name__ == \"__main__\":\n","   dataset = numpy.loadtxt(fname='/content/gdrive/My Drive/437/HW1/har.csv', delimiter=',')\n","    \n","   # test data will be every 3rd element from the dataset:\n","  \n","   testData = numpy.array([dataset[2]])\n","    \n","   #for x in range (1, int (len(dataset) / 3) ):\n","   #for x in range (1, 1000 ):\n","   for x in range (1, len(dataset) ):\n","      if (x + 1) % 3 == 0:\n","        testData = numpy.append (testData, [dataset[x]], axis = 0)\n","   '''\n","   dataset = [[2.771244718,1.784783929,0], [1.728571309,1.169761413,0],\n","              [3.678319846,2.812813570,0], [3.961043357,2.619950320,0],\n","              [2.999208922,2.209014212,0], [7.497545867,3.162953546,1],\n","              [9.00220326, 3.339047188,1], [7.444542326,0.476683375,1],\n","              [10.12493903,3.234550982,1], [6.642287351,3.319983761,1]]\n","   '''\n","   #tree = build_tree(dataset, 3)\n","   tree = build_tree(testData, 1)\n","   print_tree(tree)\n","   correct = 0\n","   total = 0\n","   #for row in dataset:\n","   for row in testData:\n","      total += 1\n","      prediction = predict(tree, row)\n","      if prediction == row[-1]:\n","        correct += 1\n","      #print('Predicted=%d, Ground truth=%d' % (prediction, row[-1]))\n","   print ('correct: ', correct, '/', total, ' correct : ', 100 * (float(correct) / total) , '% prediction success')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Tree:\n","[X84 < -0.662]\n"," [1.0]\n"," [0.0]\n","Tree:\n","[X4 < -0.567]\n"," [1.0]\n"," [0.0]\n","correct:  334 / 334  correct :  100.0 % prediction success\n"],"name":"stdout"}]}]}