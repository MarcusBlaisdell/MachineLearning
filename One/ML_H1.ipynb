{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML.H1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "XgxULieqjM0n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9fk7La4-jNze",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Homework Assignment #1**\n",
        "\n",
        "Assigned: January 8, 2019\n",
        "\n",
        "Due: January 17, 2019\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This assignment consists of four questions that require a short answer and one that requires you to generate some Python code. You can enter your answers and your code directly in a Colaboratory notebook and upload the **shareable** link for the your notebook as your homework submission.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#1.\n",
        "\n",
        "(12 points) This question focuses on the topic of data collection and feature extraction. Consider a scenario in which we want to identify the person who hand-wrote a particular message. Collect data for this machine learning scenario by writing the word \"Cougs\" ten times. Also ask a friend or classmate to write this word ten times. Capture and upload an electronic image with these signatures. Analyzing these twenty images, write descriptions of at least ten specific features (types of strokes, lines, dots) that would best discriminate your handwriting from that of your friend.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#2.\n",
        "\n",
        "(9 points) Generate decision trees that would represent the following boolean functions:\n",
        "\n",
        "\n",
        "\n",
        "*   not(A) and not(B)\n",
        "*   (A and B) or C\n",
        "*   A XOR B     (here \"XOR\" refers to exclusive or)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#3.\n",
        "\n",
        "(6 points) Assume that you are given the set of labeled training examples that appear below, where each attribute has possible values *a*, *b*, or *c*, and the target *Output* has values + or -. What is the information gain for each attribute in this dataset?\n",
        "\n",
        "\n",
        "F1 | F2 | F3\n",
        "--- | --- | ---\n",
        "a | a | a\n",
        "c | b | c\n",
        "c | a | c\n",
        "b | a | a\n",
        "a | b | c\n",
        "b | b | c\n",
        "\n",
        "\n",
        "What feature would be chosen as the root of a decision tree?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#4.\n",
        "\n",
        "(12 points) Consider the training set accuracy and test set accuracy curves plotted in the graph below as a function of the decision tree size (number of nodes in the decision tree). From the accuracy value we can compute error as (1.0 - accuracy).\n",
        "\n",
        "![](https://drive.google.com/uc?id=1LVeFRdtI0cg9es6b4E12HO0AZSgn78fj)\n",
        "\n",
        "Can you suggest a way to determine the amount of overfit that exists in the learned decision tree model based on these curves? Explain / justify your answer.\n",
        "\n",
        "Based on the graph above what size decision tree would you choose to use and why?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#5.\n",
        "\n",
        "(80 points) In this problem you are asked to become familiar with python-based decision tree code and make modifications to the code. The code is based on a structure defined at machinelearningmastery.com. *Note that all of the code you write needs to be entirely your own, not copied from another existing program or using existing libraries that perform the specified functionality.*\n",
        "\n",
        "The first thing to note here is that the function used to determine the ideal attribute for splitting is gini index, rather than information gain as we discussed in class. Instead of using the entropy measure $-p^+ log_2 p^+ - p^- log_2 p^-$, we now use the gini measure $p^+(1-p^+) + p^-(1-p^-)$ . You can test out this function by adding these lines to the bottom of the code segment:\n",
        "\n",
        "print(gini_index([[[1, 1], [1, 0]], [[1, 1], [1, 0]]], [0, 1]))\n",
        "\n",
        "print(gini_index([[[1, 0], [1, 0]], [[1, 1], [1, 1]]], [0, 1]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "z9Gq16k1izIC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Calculate the Gini index for a subset of the dataset\n",
        "def gini_index(groups, classes):\n",
        "   # count all samples at split point\n",
        "   num_instances = float(sum([len(group) for group in groups]))\n",
        "\n",
        "   gini = 0.0 # sum weighted Gini index for each group\n",
        "   for group in groups:\n",
        "      size = float(len(group))\n",
        "      if size == 0: # avoid divide by zero\n",
        "         continue\n",
        "      score = 0.0\n",
        "      # score the group based on the score for each class\n",
        "      for class_val in classes:\n",
        "         p = [row[-1] for row in group].count(class_val) / size\n",
        "         score += p * p\n",
        "      # weight the group score by its relative size\n",
        "      gini += (1.0 - score) * (size / num_instances)\n",
        "   return gini"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SbV_IyrXjIPw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we define functions that test alternative attributes for splitting a node of the tree into two children (this code defines a binary decision tree). The test_split function divides the training data into two groups, one for each child. The get_split function determines the best attribute to split by calling test_split then evaluating the resulting data subsets using gini index.\n",
        "\n",
        "You may notice an important different between this decision tree and one we discussed in class. This tree has numeric features, rather than discrete features with symbolic names. This adds a new level of complexity to the idea of splitting the tree. Instead of creating a separate child for each discrete value of the feature, we split the entire numeric range into two partitions based on a threshold value (this is the row[index] value in the select_attribute function). Data points whose value for the selected attribute is < threshold are assigned to the left child of the split node, the remaining data points are assigned to the right child. Rather than test all possible threshold values (of which there are an infinite number) to determine which yields the best gini value, only the values that are found in the actual dataset are tested.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "tWAIUJqWlODJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create child splits for a node or make a leaf node\n",
        "def split(node, max_depth, depth):\n",
        "   left, right = node['groups']\n",
        "   del(node['groups'])\n",
        "   # check for a no split\n",
        "   if not left or not right:\n",
        "      node['left'] = node['right'] = create_leaf(left + right)\n",
        "      return\n",
        "   # check for max depth\n",
        "   if depth >= max_depth:\n",
        "      node['left'], node['right'] = create_leaf(left), create_leaf(right)\n",
        "      return\n",
        "   node['left'] = select_attribute(left)\n",
        "   split(node['left'], max_depth, depth+1)\n",
        "   node['right'] = select_attribute(right)\n",
        "   split(node['right'], max_depth, depth+1)\n",
        "\n",
        "\n",
        "# split the dataset based on an attribute and attribute value\n",
        "def test_split(index, value, dataset):\n",
        "   left, right = list(), list()\n",
        "   for row in dataset:\n",
        "      if row[index] < value:\n",
        "         left.append(row)\n",
        "      else:\n",
        "         right.append(row)\n",
        "   return left, right\n",
        "\n",
        "\n",
        "# Select the best split point for a dataset\n",
        "def select_attribute(dataset):\n",
        "   class_values = list(set(row[-1] for row in dataset))\n",
        "   b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "   for index in range(len(dataset[0])-1):\n",
        "      for row in dataset:\n",
        "         groups = test_split(index, row[index], dataset)\n",
        "         gini = gini_index(groups, class_values)\n",
        "         if gini < b_score:\n",
        "            b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "   return {'index':b_index, 'value':b_value, 'groups':b_groups}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pc99NwMgcVLZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tBAmnqOdlyGK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Because the dataset we will eventually use for this assignment is quite large, we add a max_depth parameter to the tree. As the tree is built, if the depth limit is reached the node is not split further.\n",
        "Similarly, if the subset of data at the current node is homogeneous (all the same class value), there is no need for a split. In either of these cases, the current node is considered a leaf node. The create_leaf function determines the class value that will be returned for that leaf node (based on the majority class label for training data that was assigned to the leaf node)."
      ]
    },
    {
      "metadata": {
        "id": "ixDly7OPmKC5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a leaf node class value\n",
        "def create_leaf(group):\n",
        "   outcomes = [row[-1] for row in group]\n",
        "   return max(set(outcomes), key=outcomes.count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YcR92JxUhK3r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As shown in the code below, this version of the program starts with a hard-coded dataset. This dataset contains two numeric attributes and one integer class value. The main function defines the dataset, calls a function to train the decision tree and print the resulting tree structure.  You can try running this and seeing what type of tree structure is learned from the example dataset. Try varying the value of max_depth and see how this affects the tree structure."
      ]
    },
    {
      "metadata": {
        "id": "A1VHmw6QgZXi",
        "colab_type": "code",
        "outputId": "a75cba70-71b3-44a6-f8b9-651832486e7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "cell_type": "code",
      "source": [
        "# Build a decision tree\n",
        "def build_tree(train, max_depth):\n",
        "   root = select_attribute(train)\n",
        "   split(root, max_depth, 1)\n",
        "   return root\n",
        "  \n",
        "  \n",
        "# Print a decision tree\n",
        "def print_tree(node, depth=0):\n",
        "   if depth == 0:\n",
        "      print 'Tree:'\n",
        "   if isinstance(node, dict):\n",
        "      print('%s[X%d < %.3f]' % ((depth*' ', (node['index']+1), node['value'])))\n",
        "      print_tree(node['left'], depth+1)\n",
        "      print_tree(node['right'], depth+1)\n",
        "   else:\n",
        "      print('%s[%s]' % ((depth*' ', node)))\n",
        "      \n",
        "      \n",
        "if __name__ == \"__main__\":\n",
        "   dataset = [[2.771244718,1.784783929,0], [1.728571309,1.169761413,0],\n",
        "              [3.678319846,2.812813570,0], [3.961043357,2.619950320,0],\n",
        "              [2.999208922,2.209014212,0], [7.497545867,3.162953546,1],\n",
        "              [9.00220326, 3.339047188,1], [7.444542326,0.476683375,1],\n",
        "              [10.12493903,3.234550982,1], [6.642287351,3.319983761,1]]\n",
        "   tree = build_tree(dataset, 1)\n",
        "   print_tree(tree)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tree:\n",
            "[X1 < 6.642]\n",
            " [0]\n",
            " [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A-vztiJqnY7_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The procedure to learn a decision tree is now complete. To use the tree for prediction of a new data point, we define a function predict that feeds the data point (called \"row\") through the tree (rooted at \"node\"). The value of the leaf node that is reached is returned. You can try the prediction function out by adding these three lines to the end of the main function:\n",
        "\n",
        "   for row in dataset:\n",
        "      prediction = predict(tree, row)\n",
        "      print('Predicted=%d, Ground truth=%d' % (prediction, row[-1]))\n",
        "\n",
        "The predicted values will likely match the ground truth values. This is not surprising since data points are being tested that were also used to train the tree."
      ]
    },
    {
      "metadata": {
        "id": "J9yx6ndzoFU4",
        "colab_type": "code",
        "outputId": "6adccbd8-96a8-49d5-dcd4-d2ac7291272e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "cell_type": "code",
      "source": [
        "# Make a prediction with a decision tree\n",
        "def predict(node, row):\n",
        "   if row[node['index']] < node['value']:\n",
        "      if isinstance(node['left'], dict):\n",
        "         return predict(node['left'], row)\n",
        "      else:\n",
        "         return node['left']\n",
        "   else:\n",
        "      if isinstance(node['right'], dict):\n",
        "         return predict(node['right'], row)\n",
        "      else:\n",
        "         return node['right']\n",
        "\n",
        "        \n",
        "if __name__ == \"__main__\":\n",
        "   dataset = [[2.771244718,1.784783929,0], [1.728571309,1.169761413,0],\n",
        "              [3.678319846,2.812813570,0], [3.961043357,2.619950320,0],\n",
        "              [2.999208922,2.209014212,0], [7.497545867,3.162953546,1],\n",
        "              [9.00220326, 3.339047188,1], [7.444542326,0.476683375,1],\n",
        "              [10.12493903,3.234550982,1], [6.642287351,3.319983761,1]]\n",
        "   tree = build_tree(dataset, 3)\n",
        "   print_tree(tree)\n",
        "   for row in dataset:\n",
        "      prediction = predict(tree, row)\n",
        "      print('Predicted=%d, Ground truth=%d' % (prediction, row[-1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tree:\n",
            "[X1 < 6.642]\n",
            " [X1 < 2.771]\n",
            "  [X1 < 1.729]\n",
            "   [0]\n",
            "   [0]\n",
            "  [X1 < 2.771]\n",
            "   [0]\n",
            "   [0]\n",
            " [X1 < 7.498]\n",
            "  [X1 < 7.445]\n",
            "   [1]\n",
            "   [1]\n",
            "  [X1 < 7.498]\n",
            "   [1]\n",
            "   [1]\n",
            "Predicted=0, Ground truth=0\n",
            "Predicted=0, Ground truth=0\n",
            "Predicted=0, Ground truth=0\n",
            "Predicted=0, Ground truth=0\n",
            "Predicted=0, Ground truth=0\n",
            "Predicted=1, Ground truth=1\n",
            "Predicted=1, Ground truth=1\n",
            "Predicted=1, Ground truth=1\n",
            "Predicted=1, Ground truth=1\n",
            "Predicted=1, Ground truth=1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WAUoQfPboS64",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Your role:**\n",
        "\n",
        "In this homework assignment you need to make three changes to the code that has been provided.\n",
        "\n",
        "* First, replace the gini measure with the information gain measure we described in class.\n",
        "\n",
        "* Second, introduce functions that take the dataset and split into a subset that is used for training and a subset that is used for testing. The training set should represent 2/3 of the original data and testing will be the remaining 1/3 of the data. You can be as creative as you want in splitting the data into training and testing subsets. Now modify the main function to build the tree on the training data and test the predicted values on the testing data.  Instead of reporting each ground truth label, print the ratio of correctly-labeled testing data points (the predicted label matches the ground truth label) to the total number of testing data points.\n",
        "\n",
        "* Third, instead of using the hard coded dataset that is provided here, I would like you to train and test your tree on the human activity recognition dataset that we described in class. A csv version of the code is available online at http://eecs.wsu.edu/~cook/ml/hw/har.csv. As we mentioned in class, each data point in this set represents features extracted from accelerometer and gyroscope data. The two class values are 0 (representing \"walking\") and 1 (representing \"sitting\"). Use the google library and upload_files function to upload the data and the numpy library to process the csv file, as shown in class.\n",
        "\n",
        "* Lastly, answer these questions. What observations can you make about the reported performance? Is it better than random guess?"
      ]
    }
  ]
}